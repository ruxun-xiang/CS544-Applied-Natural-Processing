{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bded3d3",
   "metadata": {
    "id": "9bded3d3"
   },
   "source": [
    "# 1 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea779fb",
   "metadata": {
    "id": "bea779fb",
    "outputId": "ee0e9264-c08a-4a9e-c47a-ec88f062f720"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful. Looks great on counter Beautiful.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome &amp; Self-ness I personally have 5 days s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fabulous and worth every penny Fabulous and wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars A must if you love garlic on tomato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Better than sex Worth every penny! Buy one now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875083</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Not as sturdy as you'd think. After a month of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875084</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A Sweetheart of A Pan I've used my Le Creuset ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875085</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Ice Cream Like a Dream According to my wife, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875086</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Opens anything and everything Hoffritz has a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875087</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The more you listen, the more you hear... OK. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4874819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                             review\n",
       "0                5.0  Beautiful. Looks great on counter Beautiful.  ...\n",
       "1                5.0  Awesome & Self-ness I personally have 5 days s...\n",
       "2                5.0  Fabulous and worth every penny Fabulous and wo...\n",
       "3                5.0  Five Stars A must if you love garlic on tomato...\n",
       "4                5.0  Better than sex Worth every penny! Buy one now...\n",
       "...              ...                                                ...\n",
       "4875083          4.0  Not as sturdy as you'd think. After a month of...\n",
       "4875084          5.0  A Sweetheart of A Pan I've used my Le Creuset ...\n",
       "4875085          4.0  Ice Cream Like a Dream According to my wife, t...\n",
       "4875086          4.0  Opens anything and everything Hoffritz has a n...\n",
       "4875087          5.0  The more you listen, the more you hear... OK. ...\n",
       "\n",
       "[4874819 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# store kitchen data into dataframe\n",
    "data_path = \"./amazon_reviews_us_Kitchen_v1_00.tsv\"\n",
    "df_ori = pd.read_csv(data_path, sep='\\t', usecols=[7, 12, 13])\n",
    "\n",
    "df_ori.replace(np.inf, np.nan)\n",
    "df_ori = df_ori.dropna(axis=0, how=\"any\")\n",
    "df_ori[\"review\"] = df_ori[\"review_headline\"] + \" \" + df_ori[\"review_body\"]\n",
    "df_ori.drop(columns=[\"review_headline\", \"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2777cd8",
   "metadata": {
    "id": "f2777cd8"
   },
   "outputs": [],
   "source": [
    "# extract reviews every label 50000\n",
    "\n",
    "df_r5 = df_ori[df_ori[\"star_rating\"] == 5.0]  \n",
    "df_r4 = df_ori[df_ori[\"star_rating\"] == 4.0] \n",
    "df_r3 = df_ori[df_ori[\"star_rating\"] == 3.0]  \n",
    "df_r2 = df_ori[df_ori[\"star_rating\"] == 2.0]  \n",
    "df_r1 = df_ori[df_ori[\"star_rating\"] == 1.0]  \n",
    "\n",
    "df_r5s = df_r5.sample(n=50000)\n",
    "df_r4s = df_r4.sample(n=50000)\n",
    "df_r3s = df_r3.sample(n=50000)\n",
    "df_r2s = df_r2.sample(n=50000)\n",
    "df_r1s = df_r1.sample(n=50000)\n",
    "df_sp = pd.concat([df_r5s, df_r4s, df_r3s, df_r2s, df_r1s])\n",
    "\n",
    "label = []\n",
    "\n",
    "for star in df_sp[\"star_rating\"]:\n",
    "  if star > 3.0:\n",
    "      label.append(0)\n",
    "  elif star < 3.0:\n",
    "      label.append(1)\n",
    "  else:\n",
    "      label.append(2)\n",
    "\n",
    "df_sp[\"label\"] = label\n",
    "col_order = [\"review\", \"label\"]\n",
    "df_sp = df_sp[col_order]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb71c08",
   "metadata": {
    "id": "4eb71c08"
   },
   "outputs": [],
   "source": [
    "# save sampled data if need\n",
    "# output_path = \"./data/dataset_sampled.csv\"\n",
    "# df_sp.to_csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7eb0e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "1d7eb0e8",
    "outputId": "f0321734-81b4-4336-a35c-52bf0374c0cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898464</td>\n",
       "      <td>Five Stars Thanks!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3985118</td>\n",
       "      <td>Just what I wanted I love the way chefs crush ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2412310</td>\n",
       "      <td>So easy Very happy with this product! Bought a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1287078</td>\n",
       "      <td>Five Stars works very well and well made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1637732</td>\n",
       "      <td>This is a great tea infuser! This is a great t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>2809365</td>\n",
       "      <td>Not Happy with it I used it about 5 times and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>4512646</td>\n",
       "      <td>Great Coffee Maker, Sneaky Company I loved thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>2835983</td>\n",
       "      <td>Worse than a blender I bought this because I n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>4563097</td>\n",
       "      <td>Cuisinart? Never Again [[ASIN:B0000A1ZN7 Cuisi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>1256456</td>\n",
       "      <td>One Star The lids didn't fit. Can't use a bowl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             review  label\n",
       "0           898464                                 Five Stars Thanks!      0\n",
       "1          3985118  Just what I wanted I love the way chefs crush ...      0\n",
       "2          2412310  So easy Very happy with this product! Bought a...      0\n",
       "3          1287078           Five Stars works very well and well made      0\n",
       "4          1637732  This is a great tea infuser! This is a great t...      0\n",
       "...            ...                                                ...    ...\n",
       "249995     2809365  Not Happy with it I used it about 5 times and ...      1\n",
       "249996     4512646  Great Coffee Maker, Sneaky Company I loved thi...      1\n",
       "249997     2835983  Worse than a blender I bought this because I n...      1\n",
       "249998     4563097  Cuisinart? Never Again [[ASIN:B0000A1ZN7 Cuisi...      1\n",
       "249999     1256456  One Star The lids didn't fit. Can't use a bowl...      1\n",
       "\n",
       "[250000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load sampled data, avoid sampling again, if need\n",
    "# file_path = \"./data/dataset_sampled.csv\"\n",
    "# df_sp = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a756f9",
   "metadata": {
    "id": "f2a756f9"
   },
   "source": [
    "# 2 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tJ4aVHZDGYyY",
   "metadata": {
    "id": "tJ4aVHZDGYyY"
   },
   "source": [
    "## Import Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22b2c5",
   "metadata": {
    "id": "8b22b2c5"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# download w2v model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "BdFZPa7hG4Ip",
   "metadata": {
    "id": "BdFZPa7hG4Ip"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# load w2v model, if need\n",
    "# wv_path = \"./model/word2vec-google-news-300.gz\"\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(wv_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3679da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8e3679da",
    "outputId": "95804b45-72ae-4223-d120-509fe4d74d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55674857\n",
      "0.7821097\n"
     ]
    }
   ],
   "source": [
    "# test w2v model\n",
    "print(wv.similarity(\"excellent\", \"outstanding\"))\n",
    "print(wv.similarity(\"car\", \"vehicle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21383e93",
   "metadata": {
    "id": "21383e93"
   },
   "outputs": [],
   "source": [
    "# train the model using our own data\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "raw_sent = list(df_sp[\"review\"])\n",
    "\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    def __init__(self, review):\n",
    "        self.review = review\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sent in self.review:\n",
    "            yield utils.simple_preprocess(sent)\n",
    "\n",
    "sentences = MyCorpus(raw_sent)\n",
    "wvmodel = Word2Vec(sentences=sentences, vector_size=300, window=11, min_count=10, workers = 4)\n",
    "\n",
    "# save the model, avoiding training again\n",
    "wvwmodel.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f816fdb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f816fdb0",
    "outputId": "0950b2a8-df17-42f9-cd49-b267ebfcc872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475236\n",
      "0.68691987\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# load the model if need\n",
    "# wvmodel = gensim.models.Word2Vec.load(\"./model/word2vec.model\")\n",
    "\n",
    "# test the new model\n",
    "print(wvmodel.wv.similarity(\"excellent\", \"outstanding\"))\n",
    "print(wvmodel.wv.similarity(\"car\", \"vehicle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16344bef",
   "metadata": {
    "id": "16344bef"
   },
   "source": [
    "## Data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b423d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "25b423d2",
    "outputId": "8db1758a-9a6a-4883-a51a-30ac1263ea0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898464</td>\n",
       "      <td>five stars thanks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3985118</td>\n",
       "      <td>just what i wanted i love the way chefs crush ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2412310</td>\n",
       "      <td>so easy very happy with this product bought a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1287078</td>\n",
       "      <td>five stars works very well and well made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1637732</td>\n",
       "      <td>this is a great tea infuser this is a great te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>2809365</td>\n",
       "      <td>not happy with it i used it about times and it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>4512646</td>\n",
       "      <td>great coffee maker sneaky company i loved this...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>2835983</td>\n",
       "      <td>worse than a blender i bought this because i n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>4563097</td>\n",
       "      <td>cuisinart never again asin b a zn cuisinart dc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>1256456</td>\n",
       "      <td>one star the lids did not fit cannot use a bow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             review  label\n",
       "0           898464                                 five stars thanks       0\n",
       "1          3985118  just what i wanted i love the way chefs crush ...      0\n",
       "2          2412310  so easy very happy with this product bought a ...      0\n",
       "3          1287078           five stars works very well and well made      0\n",
       "4          1637732  this is a great tea infuser this is a great te...      0\n",
       "...            ...                                                ...    ...\n",
       "249995     2809365  not happy with it i used it about times and it...      1\n",
       "249996     4512646  great coffee maker sneaky company i loved this...      1\n",
       "249997     2835983  worse than a blender i bought this because i n...      1\n",
       "249998     4563097  cuisinart never again asin b a zn cuisinart dc...      1\n",
       "249999     1256456  one star the lids did not fit cannot use a bow...      1\n",
       "\n",
       "[250000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# do the same data preprocessing and cleaning as HW1\n",
    "def remove_non_alphabetic(text):\n",
    "    pattern = re.compile('[^a-zA-Z]')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = '[http|https]*://[a-zA-Z0-9.?#/&-_=:]*'\n",
    "    url_pattern = re.compile(pattern)\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text).get_text()\n",
    "\n",
    "def remove_space(text):\n",
    "    space_pattern = re.compile('\\s+')\n",
    "    text = re.sub(space_pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: to_lowercase(x))\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: remove_url(x))\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: remove_html(x))\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: expand_contractions(x))\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: remove_non_alphabetic(x))\n",
    "df_sp[\"review\"] = df_sp[\"review\"].apply(lambda x: remove_space(x))\n",
    "\n",
    "df_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e13831",
   "metadata": {
    "id": "a8e13831"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "df_cd = df_sp\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lem_words = []\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "df_cd[\"review\"] = df_cd[\"review\"].apply(lambda x: word_tokenize(x))\n",
    "df_cd[\"review\"] = df_cd[\"review\"].apply(lambda x: \" \".join([word for word in x if word not in stopwords]))\n",
    "\n",
    "df_cd[\"review\"] = df_cd[\"review\"].apply(lambda x: word_tokenize(x))\n",
    "df_cd[\"review\"] = df_cd[\"review\"].apply(lambda x: \" \".join([wnl.lemmatize(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6154f1",
   "metadata": {
    "id": "6b6154f1"
   },
   "outputs": [],
   "source": [
    "# load cleaned data (might have nan value, aovid to load directly)\n",
    "# file_path = \"./data/dataset_cleaned.csv\"\n",
    "# df_cd = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41b6707",
   "metadata": {
    "id": "f41b6707"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# take data for binary classification and ternery\n",
    "\n",
    "df_pos = df_cd[df_cd[\"label\"] == 0]  # positive review\n",
    "df_neg = df_cd[df_cd[\"label\"] == 1]  # negative review\n",
    "\n",
    "\n",
    "# split the data for binary and ternary cases\n",
    "df_bin = pd.concat([df_pos, df_neg])\n",
    "df_neu = df_cd[df_cd[\"label\"] == 2] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682ba53",
   "metadata": {
    "id": "4682ba53"
   },
   "source": [
    "## Convert reviews into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nXR7yOmBHaAm",
   "metadata": {
    "id": "nXR7yOmBHaAm"
   },
   "source": [
    "### Pretrained w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MUTSqg6QHdIc",
   "metadata": {
    "id": "MUTSqg6QHdIc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# get word vector in review\n",
    "\n",
    "pre_vec_bin = []\n",
    "pre_vec_neu = []\n",
    "\n",
    "# Average sentence vector\n",
    "for df in df_bin, df_neu:\n",
    "    for index, row in df.iterrows():\n",
    "        review = row[\"review\"]\n",
    "        wordcount = 0\n",
    "        vec = np.zeros(300)\n",
    "        for word in review.split(\" \"):\n",
    "            try:\n",
    "                vec += wv[word]\n",
    "                wordcount += 1\n",
    "            except KeyError as e:  # when there's no such word, ignore it\n",
    "                continue\n",
    "        if wordcount != 0:\n",
    "            vec /= wordcount\n",
    "        if df.shape[0] == 200000:\n",
    "            pre_vec_bin.append(vec)\n",
    "        elif df.shape[0] == 50000:\n",
    "            pre_vec_neu.append(vec)\n",
    "\n",
    "pre_vec_all = pre_vec_bin + pre_vec_neu\n",
    "\n",
    "# 10 word vectors\n",
    "pre_vec10_bin = []\n",
    "pre_vec10_neu = []\n",
    "vec_zero = np.zeros(300)\n",
    "\n",
    "for df in df_bin, df_neu:\n",
    "    for index, row in df.iterrows():\n",
    "        review = row[\"review\"]\n",
    "        split_review = review.split(\" \")\n",
    "        len_review = len(split_review)\n",
    "        try:\n",
    "            vec = wv[split_review[0]]\n",
    "        except:\n",
    "            vec = np.zeros(300)\n",
    "        for word in split_review[1:10]:\n",
    "            try:\n",
    "                vec = np.append(vec, wvmodel.wv[word])\n",
    "            except KeyError as e:\n",
    "                continue    \n",
    "        if len(vec) < 3000:\n",
    "            vec_zero = np.zeros(3000 - len(vec))\n",
    "            vec = np.append(vec, vec_zero)\n",
    "#         print(len(vec))\n",
    "        if df.shape[0] == 200000:\n",
    "            pre_vec10_bin.append(vec)\n",
    "        else:\n",
    "            pre_vec10_neu.append(vec)\n",
    "        \n",
    "pre_vec10_all = pre_vec10_bin + pre_vec10_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n_4CypKiQi58",
   "metadata": {
    "id": "n_4CypKiQi58"
   },
   "outputs": [],
   "source": [
    "# save pretrained vector, if need\n",
    "\n",
    "X_pre_bin = np.array(pre_vec_bin)\n",
    "# np.save(\"./pretrained/xprebin_2.npy\", X_pre_bin)\n",
    "X_pre_all = np.array(pre_vec_all)\n",
    "# np.save(\"./pretrained/xpreall2_2.npy\", X_pre_all)\n",
    "\n",
    "X_pre_bin10 = np.array(pre_vec10_bin)\n",
    "# np.save(\"./pretrained/xprebin10_2.npy\", X_pre_bin10)\n",
    "X_pre_all10 = np.array(pre_vec10_all)\n",
    "# np.save(\"./pretrained/xpreall10_2.npy\", X_pre_all10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca703c4",
   "metadata": {
    "id": "0ca703c4"
   },
   "outputs": [],
   "source": [
    "# df_bin[\"label\"]\n",
    "\n",
    "y_bin = df_bin[\"label\"]\n",
    "y_bin = np.array(y_bin.values)\n",
    "# np.save('./pretrained/ybin.npy', y_bin)\n",
    "\n",
    "y_neu = df_neu[\"label\"]\n",
    "y_neu = np.array(y_neu.values)\n",
    "\n",
    "y_all = np.append(y_bin, y_neu)\n",
    "# np.save('./pretrained/yall.npy', y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nuku-hYSHUWU",
   "metadata": {
    "id": "Nuku-hYSHUWU"
   },
   "source": [
    "### Own w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8676a",
   "metadata": {
    "id": "e8c8676a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# get word vector in review\n",
    "\n",
    "sent_vec_bin = []\n",
    "sent_vec_neu = []\n",
    "\n",
    "\n",
    "# Average sentence vector\n",
    "for df in df_bin, df_neu:\n",
    "    for index, row in df.iterrows():\n",
    "        review = row[\"review\"]\n",
    "        wordcount = 0\n",
    "        vec = np.zeros(300)\n",
    "        for word in review.split(\" \"):\n",
    "            try:\n",
    "                vec += wvmodel.wv[word]\n",
    "                wordcount += 1\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        if wordcount != 0:\n",
    "            vec /= wordcount\n",
    "        if df.shape[0] == 200000:\n",
    "            sent_vec_bin.append(vec)\n",
    "        elif df.shape[0] == 50000:\n",
    "            sent_vec_neu.append(vec)\n",
    "\n",
    "            \n",
    "sent_vec_all = sent_vec_bin + sent_vec_neu\n",
    "\n",
    "# 10 word vectors\n",
    "sent_vec10_bin = []\n",
    "sent_vec10_neu = []\n",
    "vec_zero = np.zeros(300)\n",
    "\n",
    "for df in df_bin, df_neu:\n",
    "    for index, row in df.iterrows():\n",
    "        review = row[\"review\"]\n",
    "        split_review = review.split(\" \")\n",
    "        len_review = len(split_review)\n",
    "        try:\n",
    "            vec = wvmodel.wv[split_review[0]]\n",
    "        except:\n",
    "            vec = np.zeros(300)\n",
    "        for word in split_review[1:10]:\n",
    "            try:\n",
    "                vec = np.append(vec, wvmodel.wv[word])\n",
    "            except KeyError as e:\n",
    "                continue    \n",
    "        if len(vec) < 3000:\n",
    "            vec_zero = np.zeros(3000 - len(vec))\n",
    "            vec = np.append(vec, vec_zero)\n",
    "#         print(len(vec))\n",
    "        if df.shape[0] == 200000:\n",
    "            sent_vec10_bin.append(vec)\n",
    "        else:\n",
    "            sent_vec10_neu.append(vec)\n",
    "        \n",
    "sent_vec10_all = sent_vec10_bin + sent_vec10_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce462ad",
   "metadata": {
    "id": "2ce462ad",
    "outputId": "310192e7-2c09-4d22-de7c-4eecebcd3ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3000)\n",
      "(250000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# save vectors, avoid converting again\n",
    "# own model\n",
    "# average vectors \n",
    "X_bin = np.array(sent_vec_bin)\n",
    "# np.save('./own/xbin.npy',X_bin)\n",
    "X_all = np.array(sent_vec_all)\n",
    "# np.save('./own/xall.npy',X_all)\n",
    "\n",
    "# 10 word vectors\n",
    "X10_bin = np.array(sent_vec10_bin)\n",
    "# np.save('./own/x10bin.npy',X10_bin)\n",
    "\n",
    "X10_all = np.array(sent_vec10_all)\n",
    "# np.save('./own/x10all.npy',X10_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd92e8",
   "metadata": {
    "id": "e5dd92e8"
   },
   "source": [
    "# 3 Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344bb5d",
   "metadata": {
    "id": "3344bb5d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CIwDW4EuD4ky",
   "metadata": {
    "id": "CIwDW4EuD4ky"
   },
   "source": [
    "## SVM \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZSHHZX0HMdd4",
   "metadata": {
    "id": "ZSHHZX0HMdd4"
   },
   "source": [
    "### Pretrained w2v model--m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y85ntV46SEFR",
   "metadata": {
    "id": "Y85ntV46SEFR",
    "outputId": "7ff8fe86-a19c-4102-f110-8ff1c1bdbdff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Accuracy: 0.84559375\n",
      "Precision: 0.8616378933156651\n",
      "Recall: 0.8348271601947628\n",
      "F1: 0.8480206699271016\n",
      "Test\n",
      "Accuracy: 0.847225\n",
      "Precision: 0.8621482481131604\n",
      "Recall: 0.8372488107950684\n",
      "F1: 0.8495161171168952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_pre_bin\n",
    "y = y_bin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = LinearSVC(max_iter=500)\n",
    "svm.fit(X_train, y_train)\n",
    "pred_train = svm.predict(X_train)\n",
    "print(\"Train\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_train, y_train)))\n",
    "print(\"Precision: \" + str(precision_score(pred_train, y_train)))\n",
    "print(\"Recall: \" + str(recall_score(pred_train, y_train)))\n",
    "print(\"F1: \" + str(f1_score(pred_train, y_train)))\n",
    "\n",
    "pred_test = svm.predict(X_test)\n",
    "print(\"Test\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_test, y_test)))\n",
    "print(\"Precision: \" + str(precision_score(pred_test, y_test)))\n",
    "print(\"Recall: \" + str(recall_score(pred_test, y_test)))\n",
    "print(\"F1: \" + str(f1_score(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-zmSgmaMUGt",
   "metadata": {
    "id": "m-zmSgmaMUGt"
   },
   "source": [
    "### Own w2v model--m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52852217",
   "metadata": {
    "id": "52852217",
    "outputId": "9a6f7015-5c70-4609-b14c-a0000fe0c97e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Accuracy: 0.88163125\n",
      "Precision: 0.887077619291688\n",
      "Recall: 0.8775010511216086\n",
      "F1: 0.8822633486469517\n",
      "Test\n",
      "Accuracy: 0.883275\n",
      "Precision: 0.8877392912480632\n",
      "Recall: 0.8799544193420531\n",
      "F1: 0.8838297131198527\n"
     ]
    }
   ],
   "source": [
    "X = X_bin\n",
    "y = y_bin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = LinearSVC(max_iter=500)\n",
    "svm.fit(X_train, y_train)\n",
    "pred_train = svm.predict(X_train)\n",
    "print(\"Train\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_train, y_train)))\n",
    "print(\"Precision: \" + str(precision_score(pred_train, y_train)))\n",
    "print(\"Recall: \" + str(recall_score(pred_train, y_train)))\n",
    "print(\"F1: \" + str(f1_score(pred_train, y_train)))\n",
    "\n",
    "pred_test = svm.predict(X_test)\n",
    "print(\"Test\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_test, y_test)))\n",
    "print(\"Precision: \" + str(precision_score(pred_test, y_test)))\n",
    "print(\"Recall: \" + str(recall_score(pred_test, y_test)))\n",
    "print(\"F1: \" + str(f1_score(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I4IikKfxEK5k",
   "metadata": {
    "id": "I4IikKfxEK5k"
   },
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q2F_F7BuMlky",
   "metadata": {
    "id": "Q2F_F7BuMlky"
   },
   "source": [
    "### Pretrained w2v model--m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rM8V18hjMpKj",
   "metadata": {
    "id": "rM8V18hjMpKj",
    "outputId": "1f88a174-f359-4f15-e993-1cd214abb740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Accuracy: 0.81939375\n",
      "Precision: 0.894053229657595\n",
      "Recall: 0.7778768762236241\n",
      "F1: 0.831928716418605\n",
      "Test\n",
      "Accuracy: 0.821675\n",
      "Precision: 0.8927375418603489\n",
      "Recall: 0.7817314425770309\n",
      "F1: 0.8335550110838876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X = X_pre_bin\n",
    "y = y_bin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"Train\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_train, y_train)))\n",
    "print(\"Precision: \" + str(precision_score(pred_train, y_train)))\n",
    "print(\"Recall: \" + str(recall_score(pred_train, y_train)))\n",
    "print(\"F1: \" + str(f1_score(pred_train, y_train)))\n",
    "\n",
    "pred_test = clf.predict(X_test)\n",
    "print(\"Test\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_test, y_test)))\n",
    "print(\"Precision: \" + str(precision_score(pred_test, y_test)))\n",
    "print(\"Recall: \" + str(recall_score(pred_test, y_test)))\n",
    "print(\"F1: \" + str(f1_score(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feVl8sMia0",
   "metadata": {
    "id": "96feVl8sMia0"
   },
   "source": [
    "### Own w2v model--m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9318b14",
   "metadata": {
    "id": "d9318b14",
    "outputId": "4c29cf44-8797-459f-8b85-39c4187ce950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Accuracy: 0.83746875\n",
      "Precision: 0.7855062317952821\n",
      "Recall: 0.8765798945341927\n",
      "F1: 0.8285478819845064\n",
      "Test\n",
      "Accuracy: 0.840525\n",
      "Precision: 0.7877242964962263\n",
      "Recall: 0.8808405991504583\n",
      "F1: 0.8316842133037811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "X = X_bin\n",
    "y = y_bin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "print(\"Train\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_train, y_train)))\n",
    "print(\"Precision: \" + str(precision_score(pred_train, y_train)))\n",
    "print(\"Recall: \" + str(recall_score(pred_train, y_train)))\n",
    "print(\"F1: \" + str(f1_score(pred_train, y_train)))\n",
    "\n",
    "pred_test = clf.predict(X_test)\n",
    "print(\"Test\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(pred_test, y_test)))\n",
    "print(\"Precision: \" + str(precision_score(pred_test, y_test)))\n",
    "print(\"Recall: \" + str(recall_score(pred_test, y_test)))\n",
    "print(\"F1: \" + str(f1_score(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fswO5kvuEUtj",
   "metadata": {
    "id": "fswO5kvuEUtj"
   },
   "source": [
    "# 4 Feedfoward Neueral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5HEWjaIM4Zo",
   "metadata": {
    "id": "f5HEWjaIM4Zo"
   },
   "source": [
    "## Pretrained w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21oYMeGhJJle",
   "metadata": {
    "id": "21oYMeGhJJle"
   },
   "source": [
    "### Average feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lNK7oJKnEbkJ",
   "metadata": {
    "id": "lNK7oJKnEbkJ"
   },
   "source": [
    "#### Binary case--m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ps3y2HikoPxG",
   "metadata": {
    "id": "Ps3y2HikoPxG"
   },
   "outputs": [],
   "source": [
    "# load dataset if need\n",
    "\n",
    "# X_pre_bin = np.load(\"./pretrained/xprebin.npy\")\n",
    "# y_bin = np.load(\"./pretrained/ybin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8053351",
   "metadata": {
    "id": "a8053351",
    "outputId": "e6265270-ed2c-458b-b8bb-93251afd07f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.561819\n",
      "Epoch:2 Training Loss: 0.379455\n",
      "Epoch:3 Training Loss: 0.359355\n",
      "Epoch:4 Training Loss: 0.346756\n",
      "Epoch:5 Training Loss: 0.337668\n",
      "Accuracy on the test set: 0.8408\n"
     ]
    }
   ],
   "source": [
    "# MLP Binary model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 2\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "X = torch.from_numpy(X_pre_bin).float()\n",
    "y = torch.from_numpy(y_bin).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "model = Net()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "    \n",
    "epoch = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lmf85p9qEsMd",
   "metadata": {
    "id": "Lmf85p9qEsMd"
   },
   "source": [
    "#### Ternary case--m6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K9eJO_PhopLq",
   "metadata": {
    "id": "K9eJO_PhopLq"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# X_pre_all = np.load(\"./pretrained/xpreall.npy\")\n",
    "# y_all = np.load(\"./pretrained/yall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15464694",
   "metadata": {
    "id": "15464694",
    "outputId": "7f1c3cec-87d6-4879-b066-d7b33a4272c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.861208\n",
      "Epoch:2 Training Loss: 0.764774\n",
      "Epoch:3 Training Loss: 0.745584\n",
      "Epoch:4 Training Loss: 0.726199\n",
      "Epoch:5 Training Loss: 0.694264\n",
      "Accuracy on the test set: 0.7080\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 3 # for ternery\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "X = torch.from_numpy(X_pre_all).float()\n",
    "y = torch.from_numpy(y_all).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IU3OI7XMJRuK",
   "metadata": {
    "id": "IU3OI7XMJRuK"
   },
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hJ0nAJNnJZDz",
   "metadata": {
    "id": "hJ0nAJNnJZDz"
   },
   "source": [
    "#### Binary case--m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14892465",
   "metadata": {
    "id": "14892465"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# X_pre_bin10 = np.load(\"./pretrained/xprebin10.npy\")\n",
    "# y_bin = np.load(\"./pretrained/ybin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cCn_xj0iSwbX",
   "metadata": {
    "id": "cCn_xj0iSwbX",
    "outputId": "722da807-6484-40de-efa3-ccda88a2f947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.498130\n",
      "Epoch:2 Training Loss: 0.405385\n",
      "Epoch:3 Training Loss: 0.372602\n",
      "Epoch:4 Training Loss: 0.347249\n",
      "Epoch:5 Training Loss: 0.323419\n",
      "Accuracy on the test set: 0.8312\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 2\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "\n",
    "X = torch.from_numpy(X_pre_bin10).float()\n",
    "y = torch.from_numpy(y_bin).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ilssdRrOJeTv",
   "metadata": {
    "id": "ilssdRrOJeTv"
   },
   "source": [
    "#### Ternary case--m8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc7140",
   "metadata": {
    "id": "12fc7140"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# X_pre_all10 = np.load(\"./pretrained/xpreall10.npy\")\n",
    "# y_all = np.load(\"./pretrained/yall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YfE6rSSxTC1o",
   "metadata": {
    "id": "YfE6rSSxTC1o",
    "outputId": "87ea81d6-5094-4840-93e0-5457f11a0c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.836500\n",
      "Epoch:2 Training Loss: 0.776325\n",
      "Epoch:3 Training Loss: 0.747137\n",
      "Epoch:4 Training Loss: 0.722954\n",
      "Epoch:5 Training Loss: 0.701338\n",
      "Accuracy on the test set: 0.6517\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 3\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "X = torch.from_numpy(X_pre_all10).float()\n",
    "y = torch.from_numpy(y_all).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CYRLV9-2NLWq",
   "metadata": {
    "id": "CYRLV9-2NLWq"
   },
   "source": [
    "## Own w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GP3B-PrWNO_i",
   "metadata": {
    "id": "GP3B-PrWNO_i"
   },
   "source": [
    "### Average feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eZ52u21wNTRQ",
   "metadata": {
    "id": "eZ52u21wNTRQ"
   },
   "source": [
    "#### Binary case--m9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a204b",
   "metadata": {
    "id": "b49a204b"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# X_bin = np.load(\"./own/xbin.npy\")\n",
    "# y_bin = np.load(\"./pretrained/ybin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FnJGqJWpSi9Y",
   "metadata": {
    "id": "FnJGqJWpSi9Y",
    "outputId": "b4d75684-6c00-43e9-8dfb-47217e1c9a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.446514\n",
      "Epoch:2 Training Loss: 0.315571\n",
      "Epoch:3 Training Loss: 0.295449\n",
      "Epoch:4 Training Loss: 0.285296\n",
      "Epoch:5 Training Loss: 0.277731\n",
      "Accuracy on the test set: 0.8856\n"
     ]
    }
   ],
   "source": [
    "# MLP Binary model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 2\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "X = torch.from_numpy(X_bin).float()\n",
    "y = torch.from_numpy(y_bin).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "model = Net()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    \n",
    "epoch = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvqfKuNWNVTX",
   "metadata": {
    "id": "uvqfKuNWNVTX"
   },
   "source": [
    "#### Ternary case--m10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bb78a",
   "metadata": {
    "id": "ba2bb78a"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# X_all = np.load(\"./own/xall.npy\")\n",
    "# y_all = np.load(\"./pretrained/yall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FoeYL7_rSsRl",
   "metadata": {
    "id": "FoeYL7_rSsRl",
    "outputId": "7a010fc7-339a-4189-d25f-faa2d9b0b586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.825166\n",
      "Epoch:2 Training Loss: 0.680601\n",
      "Epoch:3 Training Loss: 0.639004\n",
      "Epoch:4 Training Loss: 0.622262\n",
      "Epoch:5 Training Loss: 0.612030\n",
      "Accuracy on the test set: 0.7425\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 3\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "X = torch.from_numpy(X_all).float()\n",
    "y = torch.from_numpy(y_all).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A4_Fj5_zNXSw",
   "metadata": {
    "id": "A4_Fj5_zNXSw"
   },
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KBiPNFHJNZqy",
   "metadata": {
    "id": "KBiPNFHJNZqy"
   },
   "source": [
    "#### Binary case--m11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f97937",
   "metadata": {
    "id": "d8f97937",
    "outputId": "f023670e-fdd1-4104-9677-b093b0be3ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# X10_bin = np.load(\"./own/x10bin.npy\")\n",
    "# y_bin = np.load(\"./pretrained/ybin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pt_N8ICMTPGn",
   "metadata": {
    "id": "Pt_N8ICMTPGn",
    "outputId": "5ce85cd4-5157-4ac9-97e2-2f4baef33d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.405663\n",
      "Epoch:2 Training Loss: 0.323351\n",
      "Epoch:3 Training Loss: 0.302963\n",
      "Epoch:4 Training Loss: 0.284826\n",
      "Epoch:5 Training Loss: 0.268347\n",
      "Accuracy on the test set: 0.8594\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 2\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "X = torch.from_numpy(X10_bin).float()\n",
    "y = torch.from_numpy(y_bin).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xxrSZaM_Ndxg",
   "metadata": {
    "id": "xxrSZaM_Ndxg"
   },
   "source": [
    "#### Ternary case--m12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73344002",
   "metadata": {
    "id": "73344002"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# X10_all = np.load(\"./own/x10all.npy\")\n",
    "# y_all = np.load(\"./pretrained/yall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5BPveqQsTV9i",
   "metadata": {
    "id": "5BPveqQsTV9i",
    "outputId": "a8650307-8ec4-43b5-9754-0045d9ec48c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch:1 Training Loss: 0.757325\n",
      "Epoch:2 Training Loss: 0.648764\n",
      "Epoch:3 Training Loss: 0.621757\n",
      "Epoch:4 Training Loss: 0.602401\n",
      "Epoch:5 Training Loss: 0.583307\n",
      "Accuracy on the test set: 0.7202\n"
     ]
    }
   ],
   "source": [
    "# MLP ternary model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 50\n",
    "        hidden_3 = 3\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "\n",
    "X = torch.from_numpy(X10_all).float()\n",
    "y = torch.from_numpy(y_all).float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "params = {'batch_size':64, 'num_workers': 0, 'shuffle': True}\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "model = Net()\n",
    "print(model)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def report_acc():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss))\n",
    "    \n",
    "report_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKNZ_yyeEy9F",
   "metadata": {
    "id": "FKNZ_yyeEy9F"
   },
   "source": [
    "# 5 Recurrent Neueral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8HHIBCbE1aX",
   "metadata": {
    "id": "i8HHIBCbE1aX"
   },
   "source": [
    "## RNN tensor preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gzEaVrzvTiVS",
   "metadata": {
    "id": "gzEaVrzvTiVS"
   },
   "source": [
    "### Pretrained w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "I5GvbHJ6Y_vA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "I5GvbHJ6Y_vA",
    "outputId": "f6990b7f-3694-4a5d-b77f-062628ed1e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# compute on gpu\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5BSVdYJlTmDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5BSVdYJlTmDE",
    "outputId": "ae2cf5a3-7b69-434e-c7a6-96c37c8cd830"
   },
   "outputs": [],
   "source": [
    "# RNN tensor\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "\n",
    "\n",
    "rnn_pre_bin = []\n",
    "rnn_pre_neu = []\n",
    "\n",
    "# due to limitation of my computational resource, I cannot handle all the dataset, \n",
    "# and thus use 1/5 of it.\n",
    "df_bin_sh = shuffle(df_bin)\n",
    "for review in df_bin_sh[\"review\"][:40000]:\n",
    "#     print(review)\n",
    "    split_review = review.split(\" \")\n",
    "    len_review = len(split_review)\n",
    "    try:\n",
    "        vec = wv[split_review[0]].reshape((1,300))\n",
    "    except:\n",
    "        vec = np.zeros(300).reshape((1,300))\n",
    "    for word in split_review[1:50]:\n",
    "        try:\n",
    "            vec = np.vstack((vec, wv[word].reshape((1,300))))\n",
    "        except KeyError as e:\n",
    "            continue    \n",
    "    lines = vec.shape[0]\n",
    "    if lines < 50:\n",
    "        vec_zero = np.zeros(300).reshape((1,300))\n",
    "        for line in range(50 - lines):\n",
    "            vec = np.vstack((vec, vec_zero))\n",
    "    rnn_pre_bin.append(vec)\n",
    "    \n",
    "    \n",
    "df_neu_sh = shuffle(df_neu)\n",
    "for review in df_neu_sh[\"review\"][:10000]:\n",
    "#     print(review)\n",
    "    split_review = review.split(\" \")\n",
    "    len_review = len(split_review)\n",
    "    try:\n",
    "        vec = wv[split_review[0]].reshape((1,300))\n",
    "    except:\n",
    "        vec = np.zeros(300).reshape((1,300))\n",
    "    for word in split_review[1:50]:\n",
    "        try:\n",
    "            vec = np.vstack((vec, wv[word].reshape((1,300))))\n",
    "        except KeyError as e:\n",
    "            continue    \n",
    "    lines = vec.shape[0]\n",
    "    if lines < 50:\n",
    "        vec_zero = np.zeros(300).reshape((1,300))\n",
    "        for line in range(50 - lines):\n",
    "            vec = np.vstack((vec, vec_zero))\n",
    "    rnn_pre_neu.append(vec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Xrnn_pre_bin = np.array(rnn_pre_bin)\n",
    "# np.save('./pretrained/xrnnprebin.npy',Xrnn_pre_bin)\n",
    "\n",
    "yrnn_pre_bin = df_bin_sh[\"label\"][:40000].tolist()\n",
    "yrnn_pre_bin = np.array(yrnn_pre_bin)\n",
    "# np.save('./pretrained/yrnnprebin.npy',yrnn_pre_bin)\n",
    "\n",
    "rnn_pre_all = rnn_pre_bin + rnn_pre_neu\n",
    "\n",
    "Xrnn_pre_all = np.array(rnn_pre_all)\n",
    "# np.save('./pretrained/xrnnpreall.npy',Xrnn_pre_all)\n",
    "\n",
    "yrnn_pre_neu = df_neu_sh[\"label\"][:10000].tolist()\n",
    "\n",
    "yrnn_pre_all = yrnn_pre_bin.tolist() + yrnn_pre_neu\n",
    "yrnn_pre_all = np.array(yrnn_pre_all)\n",
    "# np.save('./pretrained/yrnnpreall.npy', yrnn_pre_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IREKaqHFTdG7",
   "metadata": {
    "id": "IREKaqHFTdG7"
   },
   "source": [
    "### Own w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b94670",
   "metadata": {
    "id": "56b94670"
   },
   "outputs": [],
   "source": [
    "# RNN tensor\n",
    "from sklearn.utils import shuffle \n",
    "import numpy as np\n",
    "rnn_vec_bin = []\n",
    "rnn_vec_neu = []\n",
    "\n",
    "df_bin_sh = shuffle(df_bin)\n",
    "for review in df_bin_sh[\"review\"][:40000]:\n",
    "#     print(review)\n",
    "    split_review = review.split(\" \")\n",
    "    len_review = len(split_review)\n",
    "    try:\n",
    "        vec = wvmodel.wv[split_review[0]].reshape((1,300))\n",
    "    except:\n",
    "        vec = np.zeros(300).reshape((1,300))\n",
    "    for word in split_review[1:50]:\n",
    "        try:\n",
    "            vec = np.vstack((vec, wvmodel.wv[word].reshape((1,300))))\n",
    "        except KeyError as e:\n",
    "            continue    \n",
    "    lines = vec.shape[0]\n",
    "    if lines < 50:\n",
    "        vec_zero = np.zeros(300).reshape((1,300))\n",
    "        for line in range(50 - lines):\n",
    "            vec = np.vstack((vec, vec_zero))\n",
    "    rnn_vec_bin.append(vec)\n",
    "    \n",
    "    \n",
    "df_neu_sh = shuffle(df_neu)\n",
    "for review in df_neu_sh[\"review\"][:10000]:\n",
    "#     print(review)\n",
    "    split_review = review.split(\" \")\n",
    "    len_review = len(split_review)\n",
    "    try:\n",
    "        vec = wvmodel.wv[split_review[0]].reshape((1,300))\n",
    "    except:\n",
    "        vec = np.zeros(300).reshape((1,300))\n",
    "    for word in split_review[1:50]:\n",
    "        try:\n",
    "            vec = np.vstack((vec, wvmodel.wv[word].reshape((1,300))))\n",
    "        except KeyError as e:\n",
    "            continue    \n",
    "    lines = vec.shape[0]\n",
    "    if lines < 50:\n",
    "        vec_zero = np.zeros(300).reshape((1,300))\n",
    "        for line in range(50 - lines):\n",
    "            vec = np.vstack((vec, vec_zero))\n",
    "    rnn_vec_neu.append(vec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Xrnn_bin = np.array(rnn_vec_bin)\n",
    "# np.save('./own/xrnnbin.npy',Xrnn_bin)\n",
    "\n",
    "yrnn_bin = df_bin_sh[\"label\"][:40000].tolist()\n",
    "yrnn_bin = np.array(yrnn_bin)\n",
    "# np.save('./own/yrnnbin.npy',yrnn_bin)\n",
    "\n",
    "rnn_vec_all = rnn_vec_bin + rnn_vec_neu\n",
    "\n",
    "\n",
    "Xrnn_all = np.array(rnn_vec_all)\n",
    "# np.save('./own/xrnnall.npy',Xrnn_all)\n",
    "\n",
    "yrnn_neu = df_neu_sh[\"label\"][:10000].tolist()\n",
    "\n",
    "yrnn_all = yrnn_bin.tolist() + yrnn_neu\n",
    "yrnn_all = np.array(yrnn_all)\n",
    "# np.save('./own/yrnnall.npy', yrnn_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRMzzUBDN-sN",
   "metadata": {
    "id": "LRMzzUBDN-sN"
   },
   "source": [
    "## Pretrained w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1FoufOP7OQn5",
   "metadata": {
    "id": "1FoufOP7OQn5"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8YwadJ9E8ZA",
   "metadata": {
    "id": "W8YwadJ9E8ZA"
   },
   "source": [
    "#### Binary case--m13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "G2JhrDotSeDe",
   "metadata": {
    "id": "G2JhrDotSeDe"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Pp2VjznTf5dY",
   "metadata": {
    "id": "Pp2VjznTf5dY"
   },
   "outputs": [],
   "source": [
    "# Xrnn_pre_bin = np.load(\"./pretrained/xrnnprebin.npy\")\n",
    "# yrnn_pre_bin = np.load(\"./pretrained/yrnnprebin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a00b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "23a00b7b",
    "outputId": "f7c3385e-ecfd-4ff1-883d-d2032680d1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 0.695737 Accuracy on the test set: 0.5075\n",
      "Epoch:2 Training Loss: 0.692722 Accuracy on the test set: 0.5015\n",
      "Epoch:3 Training Loss: 0.692596 Accuracy on the test set: 0.5008\n",
      "Epoch:4 Training Loss: 0.692453 Accuracy on the test set: 0.501\n",
      "Epoch:5 Training Loss: 0.692269 Accuracy on the test set: 0.5672\n",
      "Epoch:6 Training Loss: 0.692021 Accuracy on the test set: 0.5625\n",
      "Epoch:7 Training Loss: 0.691666 Accuracy on the test set: 0.5621\n",
      "Epoch:8 Training Loss: 0.691111 Accuracy on the test set: 0.5603\n",
      "Epoch:9 Training Loss: 0.690116 Accuracy on the test set: 0.5596\n",
      "Epoch:10 Training Loss: 0.687813 Accuracy on the test set: 0.5647\n",
      "Epoch:11 Training Loss: 0.680633 Accuracy on the test set: 0.5805\n",
      "Epoch:12 Training Loss: 0.674364 Accuracy on the test set: 0.584\n",
      "Epoch:13 Training Loss: 0.662894 Accuracy on the test set: 0.6598\n",
      "Epoch:14 Training Loss: 0.618979 Accuracy on the test set: 0.7302\n",
      "Epoch:15 Training Loss: 0.583634 Accuracy on the test set: 0.6668\n",
      "Epoch:16 Training Loss: 0.566630 Accuracy on the test set: 0.7096\n",
      "Epoch:17 Training Loss: 0.552501 Accuracy on the test set: 0.7093\n",
      "Epoch:18 Training Loss: 0.544772 Accuracy on the test set: 0.7262\n",
      "Epoch:19 Training Loss: 0.538222 Accuracy on the test set: 0.779\n",
      "Epoch:20 Training Loss: 0.534333 Accuracy on the test set: 0.744\n",
      "Epoch:21 Training Loss: 0.530581 Accuracy on the test set: 0.7506\n",
      "Epoch:22 Training Loss: 0.525928 Accuracy on the test set: 0.7727\n",
      "Epoch:23 Training Loss: 0.524704 Accuracy on the test set: 0.7682\n",
      "Epoch:24 Training Loss: 0.519275 Accuracy on the test set: 0.7856\n",
      "Epoch:25 Training Loss: 0.516097 Accuracy on the test set: 0.7766\n",
      "Epoch:26 Training Loss: 0.508795 Accuracy on the test set: 0.7707\n",
      "Epoch:27 Training Loss: 0.491097 Accuracy on the test set: 0.787\n",
      "Epoch:28 Training Loss: 0.480829 Accuracy on the test set: 0.7927\n",
      "Epoch:29 Training Loss: 0.477382 Accuracy on the test set: 0.7797\n",
      "Epoch:30 Training Loss: 0.472727 Accuracy on the test set: 0.7726\n",
      "Epoch:31 Training Loss: 0.470644 Accuracy on the test set: 0.7852\n",
      "Epoch:32 Training Loss: 0.468371 Accuracy on the test set: 0.7872\n",
      "Epoch:33 Training Loss: 0.466742 Accuracy on the test set: 0.7825\n",
      "Epoch:34 Training Loss: 0.465116 Accuracy on the test set: 0.7877\n",
      "Epoch:35 Training Loss: 0.461533 Accuracy on the test set: 0.7901\n",
      "Epoch:36 Training Loss: 0.461319 Accuracy on the test set: 0.7882\n",
      "Epoch:37 Training Loss: 0.458350 Accuracy on the test set: 0.7903\n",
      "Epoch:38 Training Loss: 0.459078 Accuracy on the test set: 0.7868\n",
      "Epoch:39 Training Loss: 0.457108 Accuracy on the test set: 0.7876\n",
      "Epoch:40 Training Loss: 0.455481 Accuracy on the test set: 0.7885\n",
      "Epoch:41 Training Loss: 0.454294 Accuracy on the test set: 0.7905\n",
      "Epoch:42 Training Loss: 0.454287 Accuracy on the test set: 0.785\n",
      "Epoch:43 Training Loss: 0.453008 Accuracy on the test set: 0.7898\n",
      "Epoch:44 Training Loss: 0.452031 Accuracy on the test set: 0.7891\n",
      "Epoch:45 Training Loss: 0.452383 Accuracy on the test set: 0.7838\n",
      "Epoch:46 Training Loss: 0.450916 Accuracy on the test set: 0.7895\n",
      "Epoch:47 Training Loss: 0.448588 Accuracy on the test set: 0.7838\n",
      "Epoch:48 Training Loss: 0.446909 Accuracy on the test set: 0.7837\n",
      "Epoch:49 Training Loss: 0.447171 Accuracy on the test set: 0.7912\n",
      "Epoch:50 Training Loss: 0.448090 Accuracy on the test set: 0.7865\n",
      "Epoch:51 Training Loss: 0.446302 Accuracy on the test set: 0.7931\n",
      "Epoch:52 Training Loss: 0.445533 Accuracy on the test set: 0.792\n",
      "Epoch:53 Training Loss: 0.445416 Accuracy on the test set: 0.7943\n",
      "Epoch:54 Training Loss: 0.444195 Accuracy on the test set: 0.7886\n",
      "Epoch:55 Training Loss: 0.444038 Accuracy on the test set: 0.7868\n",
      "Epoch:56 Training Loss: 0.443764 Accuracy on the test set: 0.7937\n",
      "Epoch:57 Training Loss: 0.441975 Accuracy on the test set: 0.7946\n",
      "Epoch:58 Training Loss: 0.443159 Accuracy on the test set: 0.79\n",
      "Epoch:59 Training Loss: 0.442085 Accuracy on the test set: 0.7946\n",
      "Epoch:60 Training Loss: 0.442244 Accuracy on the test set: 0.8005\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 2\n",
    "epoch = 60\n",
    "lr = 0.003\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_pre_bin\n",
    "y = yrnn_pre_bin\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rQM50R-FAl_",
   "metadata": {
    "id": "0rQM50R-FAl_"
   },
   "source": [
    "#### Ternary case--m14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ATyOlyQfYPV",
   "metadata": {
    "id": "2ATyOlyQfYPV"
   },
   "outputs": [],
   "source": [
    "# Xrnn_pre_all = np.load(\"./pretrained/xrnnpreall.npy\")\n",
    "# yrnn_pre_all = np.load(\"./pretrained/yrnnpreall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z0lNGeVyV-Jr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0lNGeVyV-Jr",
    "outputId": "82a0466b-1728-4ddb-e7c1-39c78084fa2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 1.064453. Accuracy on the test set: 0.4102\n",
      "Epoch:2 Training Loss: 1.055470. Accuracy on the test set: 0.4126\n",
      "Epoch:3 Training Loss: 1.055045. Accuracy on the test set: 0.4134\n",
      "Epoch:4 Training Loss: 1.054853. Accuracy on the test set: 0.4156\n",
      "Epoch:5 Training Loss: 1.054681. Accuracy on the test set: 0.4158\n",
      "Epoch:6 Training Loss: 1.054515. Accuracy on the test set: 0.4172\n",
      "Epoch:7 Training Loss: 1.054355. Accuracy on the test set: 0.4176\n",
      "Epoch:8 Training Loss: 1.054199. Accuracy on the test set: 0.418\n",
      "Epoch:9 Training Loss: 1.054046. Accuracy on the test set: 0.4176\n",
      "Epoch:10 Training Loss: 1.053895. Accuracy on the test set: 0.417\n",
      "Epoch:11 Training Loss: 1.053746. Accuracy on the test set: 0.4174\n",
      "Epoch:12 Training Loss: 1.053598. Accuracy on the test set: 0.4182\n",
      "Epoch:13 Training Loss: 1.053450. Accuracy on the test set: 0.4188\n",
      "Epoch:14 Training Loss: 1.053303. Accuracy on the test set: 0.4194\n",
      "Epoch:15 Training Loss: 1.053156. Accuracy on the test set: 0.4182\n",
      "Epoch:16 Training Loss: 1.053008. Accuracy on the test set: 0.4196\n",
      "Epoch:17 Training Loss: 1.052859. Accuracy on the test set: 0.4194\n",
      "Epoch:18 Training Loss: 1.052709. Accuracy on the test set: 0.4194\n",
      "Epoch:19 Training Loss: 1.052557. Accuracy on the test set: 0.42\n",
      "Epoch:20 Training Loss: 1.052404. Accuracy on the test set: 0.421\n",
      "Epoch:21 Training Loss: 1.052249. Accuracy on the test set: 0.4234\n",
      "Epoch:22 Training Loss: 1.052091. Accuracy on the test set: 0.424\n",
      "Epoch:23 Training Loss: 1.051931. Accuracy on the test set: 0.4236\n",
      "Epoch:24 Training Loss: 1.051768. Accuracy on the test set: 0.4244\n",
      "Epoch:25 Training Loss: 1.051602. Accuracy on the test set: 0.4244\n",
      "Epoch:26 Training Loss: 1.051433. Accuracy on the test set: 0.4244\n",
      "Epoch:27 Training Loss: 1.051259. Accuracy on the test set: 0.4252\n",
      "Epoch:28 Training Loss: 1.051082. Accuracy on the test set: 0.426\n",
      "Epoch:29 Training Loss: 1.050899. Accuracy on the test set: 0.4264\n",
      "Epoch:30 Training Loss: 1.050711. Accuracy on the test set: 0.4272\n",
      "Epoch:31 Training Loss: 1.050515. Accuracy on the test set: 0.4286\n",
      "Epoch:32 Training Loss: 1.050311. Accuracy on the test set: 0.4292\n",
      "Epoch:33 Training Loss: 1.050096. Accuracy on the test set: 0.4304\n",
      "Epoch:34 Training Loss: 1.049868. Accuracy on the test set: 0.4312\n",
      "Epoch:35 Training Loss: 1.049622. Accuracy on the test set: 0.4318\n",
      "Epoch:36 Training Loss: 1.049354. Accuracy on the test set: 0.432\n",
      "Epoch:37 Training Loss: 1.049054. Accuracy on the test set: 0.4334\n",
      "Epoch:38 Training Loss: 1.048709. Accuracy on the test set: 0.435\n",
      "Epoch:39 Training Loss: 1.048292. Accuracy on the test set: 0.4348\n",
      "Epoch:40 Training Loss: 1.047747. Accuracy on the test set: 0.434\n",
      "Epoch:41 Training Loss: 1.046906. Accuracy on the test set: 0.439\n",
      "Epoch:42 Training Loss: 1.044195. Accuracy on the test set: 0.4844\n",
      "Epoch:43 Training Loss: 1.015189. Accuracy on the test set: 0.5682\n",
      "Epoch:44 Training Loss: 0.992222. Accuracy on the test set: 0.5828\n",
      "Epoch:45 Training Loss: 0.979781. Accuracy on the test set: 0.5966\n",
      "Epoch:46 Training Loss: 0.967562. Accuracy on the test set: 0.6034\n",
      "Epoch:47 Training Loss: 0.960630. Accuracy on the test set: 0.6086\n",
      "Epoch:48 Training Loss: 0.953946. Accuracy on the test set: 0.6124\n",
      "Epoch:49 Training Loss: 0.947185. Accuracy on the test set: 0.6174\n",
      "Epoch:50 Training Loss: 0.944009. Accuracy on the test set: 0.6192\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 128\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 3\n",
    "epoch = 60\n",
    "lr = 0.003\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_pre_all\n",
    "y = yrnn_pre_all\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}.'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A-o_GkGZFRvm",
   "metadata": {
    "id": "A-o_GkGZFRvm"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvVF1SdlFUF9",
   "metadata": {
    "id": "uvVF1SdlFUF9"
   },
   "source": [
    "#### Binary case--m15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GHtr8Ecp-DhO",
   "metadata": {
    "id": "GHtr8Ecp-DhO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Xrnn_pre_bin = np.load(\"./pretrained/xrnnprebin.npy\")\n",
    "# yrnn_pre_bin = np.load(\"./pretrained/yrnnprebin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6AmaxT5-yZT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6AmaxT5-yZT",
    "outputId": "c46ef895-3c2e-4596-959d-641cc7e58552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XyLUdinHWJiy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyLUdinHWJiy",
    "outputId": "05bea13c-d5e6-49bc-9866-7c9dcf6c158f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 0.693339. Accuracy on the test set: 0.5147\n",
      "Epoch:2 Training Loss: 0.692461. Accuracy on the test set: 0.5172\n",
      "Epoch:3 Training Loss: 0.691825. Accuracy on the test set: 0.5205\n",
      "Epoch:4 Training Loss: 0.691265. Accuracy on the test set: 0.5227\n",
      "Epoch:5 Training Loss: 0.690765. Accuracy on the test set: 0.5247\n",
      "Epoch:6 Training Loss: 0.690313. Accuracy on the test set: 0.5247\n",
      "Epoch:7 Training Loss: 0.689905. Accuracy on the test set: 0.5247\n",
      "Epoch:8 Training Loss: 0.689532. Accuracy on the test set: 0.5242\n",
      "Epoch:9 Training Loss: 0.689189. Accuracy on the test set: 0.5255\n",
      "Epoch:10 Training Loss: 0.688870. Accuracy on the test set: 0.526\n",
      "Epoch:11 Training Loss: 0.688570. Accuracy on the test set: 0.5267\n",
      "Epoch:12 Training Loss: 0.688283. Accuracy on the test set: 0.527\n",
      "Epoch:13 Training Loss: 0.688005. Accuracy on the test set: 0.5272\n",
      "Epoch:14 Training Loss: 0.687731. Accuracy on the test set: 0.5277\n",
      "Epoch:15 Training Loss: 0.687458. Accuracy on the test set: 0.5282\n",
      "Epoch:16 Training Loss: 0.687184. Accuracy on the test set: 0.528\n",
      "Epoch:17 Training Loss: 0.686904. Accuracy on the test set: 0.5285\n",
      "Epoch:18 Training Loss: 0.686619. Accuracy on the test set: 0.5287\n",
      "Epoch:19 Training Loss: 0.686325. Accuracy on the test set: 0.529\n",
      "Epoch:20 Training Loss: 0.686022. Accuracy on the test set: 0.529\n",
      "Epoch:21 Training Loss: 0.685706. Accuracy on the test set: 0.5282\n",
      "Epoch:22 Training Loss: 0.685378. Accuracy on the test set: 0.5285\n",
      "Epoch:23 Training Loss: 0.685033. Accuracy on the test set: 0.5295\n",
      "Epoch:24 Training Loss: 0.684672. Accuracy on the test set: 0.5312\n",
      "Epoch:25 Training Loss: 0.684289. Accuracy on the test set: 0.5315\n",
      "Epoch:26 Training Loss: 0.683883. Accuracy on the test set: 0.5327\n",
      "Epoch:27 Training Loss: 0.683449. Accuracy on the test set: 0.5332\n",
      "Epoch:28 Training Loss: 0.682982. Accuracy on the test set: 0.5352\n",
      "Epoch:29 Training Loss: 0.682476. Accuracy on the test set: 0.5362\n",
      "Epoch:30 Training Loss: 0.681919. Accuracy on the test set: 0.5382\n",
      "Epoch:31 Training Loss: 0.681301. Accuracy on the test set: 0.539\n",
      "Epoch:32 Training Loss: 0.680599. Accuracy on the test set: 0.5392\n",
      "Epoch:33 Training Loss: 0.679784. Accuracy on the test set: 0.54\n",
      "Epoch:34 Training Loss: 0.678803. Accuracy on the test set: 0.5447\n",
      "Epoch:35 Training Loss: 0.677554. Accuracy on the test set: 0.5485\n",
      "Epoch:36 Training Loss: 0.675803. Accuracy on the test set: 0.553\n",
      "Epoch:37 Training Loss: 0.672777. Accuracy on the test set: 0.565\n",
      "Epoch:38 Training Loss: 0.655452. Accuracy on the test set: 0.7327\n",
      "Epoch:39 Training Loss: 0.573986. Accuracy on the test set: 0.7845\n",
      "Epoch:40 Training Loss: 0.540888. Accuracy on the test set: 0.7872\n",
      "Epoch:41 Training Loss: 0.524601. Accuracy on the test set: 0.7962\n",
      "Epoch:42 Training Loss: 0.514568. Accuracy on the test set: 0.8007\n",
      "Epoch:43 Training Loss: 0.507488. Accuracy on the test set: 0.8045\n",
      "Epoch:44 Training Loss: 0.502033. Accuracy on the test set: 0.805\n",
      "Epoch:45 Training Loss: 0.497555. Accuracy on the test set: 0.8065\n",
      "Epoch:46 Training Loss: 0.493713. Accuracy on the test set: 0.807\n",
      "Epoch:47 Training Loss: 0.490301. Accuracy on the test set: 0.8065\n",
      "Epoch:48 Training Loss: 0.487176. Accuracy on the test set: 0.808\n",
      "Epoch:49 Training Loss: 0.484221. Accuracy on the test set: 0.8092\n",
      "Epoch:50 Training Loss: 0.481338. Accuracy on the test set: 0.81\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 2\n",
    "epoch = 50\n",
    "lr = 0.01\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_pre_bin\n",
    "y = yrnn_pre_bin\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories)\n",
    "rnn.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}.'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W5Si3FxDFWdU",
   "metadata": {
    "id": "W5Si3FxDFWdU"
   },
   "source": [
    "#### Ternary case--m16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YsgkIaGWH2Hj",
   "metadata": {
    "id": "YsgkIaGWH2Hj"
   },
   "outputs": [],
   "source": [
    "# Xrnn_pre_all = np.load(\"./pretrained/xrnnpreall.npy\")\n",
    "# yrnn_pre_all = np.load(\"./pretrained/yrnnpreall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040ef6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6040ef6e",
    "outputId": "98f5fddd-fa2f-45fd-d4c9-8ccaef5bdf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 1.061493. Accuracy on the test set: 0.4154\n",
      "Epoch:2 Training Loss: 1.054421. Accuracy on the test set: 0.4224\n",
      "Epoch:3 Training Loss: 1.053701. Accuracy on the test set: 0.4266\n",
      "Epoch:4 Training Loss: 1.053140. Accuracy on the test set: 0.428\n",
      "Epoch:5 Training Loss: 1.052667. Accuracy on the test set: 0.4286\n",
      "Epoch:6 Training Loss: 1.052259. Accuracy on the test set: 0.4292\n",
      "Epoch:7 Training Loss: 1.051900. Accuracy on the test set: 0.4294\n",
      "Epoch:8 Training Loss: 1.051581. Accuracy on the test set: 0.4292\n",
      "Epoch:9 Training Loss: 1.051290. Accuracy on the test set: 0.4296\n",
      "Epoch:10 Training Loss: 1.051020. Accuracy on the test set: 0.4298\n",
      "Epoch:11 Training Loss: 1.050766. Accuracy on the test set: 0.43\n",
      "Epoch:12 Training Loss: 1.050521. Accuracy on the test set: 0.431\n",
      "Epoch:13 Training Loss: 1.050280. Accuracy on the test set: 0.4314\n",
      "Epoch:14 Training Loss: 1.050041. Accuracy on the test set: 0.431\n",
      "Epoch:15 Training Loss: 1.049801. Accuracy on the test set: 0.4308\n",
      "Epoch:16 Training Loss: 1.049557. Accuracy on the test set: 0.431\n",
      "Epoch:17 Training Loss: 1.049308. Accuracy on the test set: 0.4312\n",
      "Epoch:18 Training Loss: 1.049051. Accuracy on the test set: 0.4314\n",
      "Epoch:19 Training Loss: 1.048786. Accuracy on the test set: 0.4324\n",
      "Epoch:20 Training Loss: 1.048511. Accuracy on the test set: 0.433\n",
      "Epoch:21 Training Loss: 1.048224. Accuracy on the test set: 0.4334\n",
      "Epoch:22 Training Loss: 1.047925. Accuracy on the test set: 0.434\n",
      "Epoch:23 Training Loss: 1.047610. Accuracy on the test set: 0.435\n",
      "Epoch:24 Training Loss: 1.047279. Accuracy on the test set: 0.436\n",
      "Epoch:25 Training Loss: 1.046928. Accuracy on the test set: 0.4366\n",
      "Epoch:26 Training Loss: 1.046554. Accuracy on the test set: 0.4366\n",
      "Epoch:27 Training Loss: 1.046152. Accuracy on the test set: 0.4382\n",
      "Epoch:28 Training Loss: 1.045716. Accuracy on the test set: 0.4388\n",
      "Epoch:29 Training Loss: 1.045239. Accuracy on the test set: 0.4384\n",
      "Epoch:30 Training Loss: 1.044708. Accuracy on the test set: 0.438\n",
      "Epoch:31 Training Loss: 1.044106. Accuracy on the test set: 0.4396\n",
      "Epoch:32 Training Loss: 1.043402. Accuracy on the test set: 0.4424\n",
      "Epoch:33 Training Loss: 1.042547. Accuracy on the test set: 0.4434\n",
      "Epoch:34 Training Loss: 1.041439. Accuracy on the test set: 0.4476\n",
      "Epoch:35 Training Loss: 1.039823. Accuracy on the test set: 0.4524\n",
      "Epoch:36 Training Loss: 1.036694. Accuracy on the test set: 0.468\n",
      "Epoch:37 Training Loss: 1.005554. Accuracy on the test set: 0.5848\n",
      "Epoch:38 Training Loss: 0.950488. Accuracy on the test set: 0.6148\n",
      "Epoch:39 Training Loss: 0.929120. Accuracy on the test set: 0.6212\n",
      "Epoch:40 Training Loss: 0.916705. Accuracy on the test set: 0.6242\n",
      "Epoch:41 Training Loss: 0.908042. Accuracy on the test set: 0.6264\n",
      "Epoch:42 Training Loss: 0.901530. Accuracy on the test set: 0.6278\n",
      "Epoch:43 Training Loss: 0.896358. Accuracy on the test set: 0.6324\n",
      "Epoch:44 Training Loss: 0.892073. Accuracy on the test set: 0.6346\n",
      "Epoch:45 Training Loss: 0.888416. Accuracy on the test set: 0.636\n",
      "Epoch:46 Training Loss: 0.885221. Accuracy on the test set: 0.6356\n",
      "Epoch:47 Training Loss: 0.882371. Accuracy on the test set: 0.6364\n",
      "Epoch:48 Training Loss: 0.879779. Accuracy on the test set: 0.6362\n",
      "Epoch:49 Training Loss: 0.877368. Accuracy on the test set: 0.636\n",
      "Epoch:50 Training Loss: 0.875070. Accuracy on the test set: 0.6372\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 32\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 3\n",
    "epoch = 50\n",
    "lr = 0.01\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_pre_all\n",
    "y = yrnn_pre_all\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories)\n",
    "rnn.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}.'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EZYExMwbOZWt",
   "metadata": {
    "id": "EZYExMwbOZWt"
   },
   "source": [
    "# Own w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4BW9RmoZPJOh",
   "metadata": {
    "id": "4BW9RmoZPJOh"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZpWXxGxePMl2",
   "metadata": {
    "id": "ZpWXxGxePMl2"
   },
   "source": [
    "#### Binary case--m17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "HKJDujrOJqB2",
   "metadata": {
    "id": "HKJDujrOJqB2"
   },
   "outputs": [],
   "source": [
    "# Xrnn_bin = np.load(\"./own/xrnnbin.npy\")\n",
    "# yrnn_bin = np.load(\"./own/yrnnbin.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FU7DL255WVus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "FU7DL255WVus",
    "outputId": "4917e166-4f16-4eda-a07b-770bd799212c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 0.694210 Accuracy on the test set: 0.5258\n",
      "Epoch:2 Training Loss: 0.693704 Accuracy on the test set: 0.5291\n",
      "Epoch:3 Training Loss: 0.693312 Accuracy on the test set: 0.5308\n",
      "Epoch:4 Training Loss: 0.692932 Accuracy on the test set: 0.5313\n",
      "Epoch:5 Training Loss: 0.692540 Accuracy on the test set: 0.534\n",
      "Epoch:6 Training Loss: 0.692123 Accuracy on the test set: 0.5352\n",
      "Epoch:7 Training Loss: 0.691669 Accuracy on the test set: 0.5365\n",
      "Epoch:8 Training Loss: 0.691167 Accuracy on the test set: 0.5395\n",
      "Epoch:9 Training Loss: 0.690605 Accuracy on the test set: 0.542\n",
      "Epoch:10 Training Loss: 0.689970 Accuracy on the test set: 0.543\n",
      "Epoch:11 Training Loss: 0.689250 Accuracy on the test set: 0.5438\n",
      "Epoch:12 Training Loss: 0.688437 Accuracy on the test set: 0.5452\n",
      "Epoch:13 Training Loss: 0.687528 Accuracy on the test set: 0.5452\n",
      "Epoch:14 Training Loss: 0.686529 Accuracy on the test set: 0.5475\n",
      "Epoch:15 Training Loss: 0.685462 Accuracy on the test set: 0.5515\n",
      "Epoch:16 Training Loss: 0.684363 Accuracy on the test set: 0.5552\n",
      "Epoch:17 Training Loss: 0.683277 Accuracy on the test set: 0.5566\n",
      "Epoch:18 Training Loss: 0.682249 Accuracy on the test set: 0.5592\n",
      "Epoch:19 Training Loss: 0.681309 Accuracy on the test set: 0.5623\n",
      "Epoch:20 Training Loss: 0.680462 Accuracy on the test set: 0.5638\n",
      "Epoch:21 Training Loss: 0.679693 Accuracy on the test set: 0.5676\n",
      "Epoch:22 Training Loss: 0.678969 Accuracy on the test set: 0.5688\n",
      "Epoch:23 Training Loss: 0.678242 Accuracy on the test set: 0.571\n",
      "Epoch:24 Training Loss: 0.677450 Accuracy on the test set: 0.5725\n",
      "Epoch:25 Training Loss: 0.676480 Accuracy on the test set: 0.575\n",
      "Epoch:26 Training Loss: 0.674991 Accuracy on the test set: 0.5826\n",
      "Epoch:27 Training Loss: 0.670902 Accuracy on the test set: 0.595\n",
      "Epoch:28 Training Loss: 0.663700 Accuracy on the test set: 0.6055\n",
      "Epoch:29 Training Loss: 0.656380 Accuracy on the test set: 0.6152\n",
      "Epoch:30 Training Loss: 0.649491 Accuracy on the test set: 0.6248\n",
      "Epoch:31 Training Loss: 0.644155 Accuracy on the test set: 0.6301\n",
      "Epoch:32 Training Loss: 0.639001 Accuracy on the test set: 0.6356\n",
      "Epoch:33 Training Loss: 0.634722 Accuracy on the test set: 0.6413\n",
      "Epoch:34 Training Loss: 0.631738 Accuracy on the test set: 0.6433\n",
      "Epoch:35 Training Loss: 0.629118 Accuracy on the test set: 0.6458\n",
      "Epoch:36 Training Loss: 0.626320 Accuracy on the test set: 0.6478\n",
      "Epoch:37 Training Loss: 0.623815 Accuracy on the test set: 0.6523\n",
      "Epoch:38 Training Loss: 0.621267 Accuracy on the test set: 0.6557\n",
      "Epoch:39 Training Loss: 0.619152 Accuracy on the test set: 0.6586\n",
      "Epoch:40 Training Loss: 0.616854 Accuracy on the test set: 0.6588\n",
      "Epoch:41 Training Loss: 0.614729 Accuracy on the test set: 0.6606\n",
      "Epoch:42 Training Loss: 0.612652 Accuracy on the test set: 0.662\n",
      "Epoch:43 Training Loss: 0.611143 Accuracy on the test set: 0.6631\n",
      "Epoch:44 Training Loss: 0.610265 Accuracy on the test set: 0.6673\n",
      "Epoch:45 Training Loss: 0.608543 Accuracy on the test set: 0.667\n",
      "Epoch:46 Training Loss: 0.606464 Accuracy on the test set: 0.6701\n",
      "Epoch:47 Training Loss: 0.607233 Accuracy on the test set: 0.6713\n",
      "Epoch:48 Training Loss: 0.604030 Accuracy on the test set: 0.6708\n",
      "Epoch:49 Training Loss: 0.603134 Accuracy on the test set: 0.6731\n",
      "Epoch:50 Training Loss: 0.602533 Accuracy on the test set: 0.6727\n",
      "Epoch:51 Training Loss: 0.602155 Accuracy on the test set: 0.6765\n",
      "Epoch:52 Training Loss: 0.602933 Accuracy on the test set: 0.6747\n",
      "Epoch:53 Training Loss: 0.604596 Accuracy on the test set: 0.6802\n",
      "Epoch:54 Training Loss: 0.603377 Accuracy on the test set: 0.6771\n",
      "Epoch:55 Training Loss: 0.600974 Accuracy on the test set: 0.6833\n",
      "Epoch:56 Training Loss: 0.654572 Accuracy on the test set: 0.6302\n",
      "Epoch:57 Training Loss: 0.625907 Accuracy on the test set: 0.6848\n",
      "Epoch:58 Training Loss: 0.617564 Accuracy on the test set: 0.6856\n",
      "Epoch:59 Training Loss: 0.619004 Accuracy on the test set: 0.6542\n",
      "Epoch:60 Training Loss: 0.618929 Accuracy on the test set: 0.6778\n",
      "Epoch:61 Training Loss: 0.612837 Accuracy on the test set: 0.6852\n",
      "Epoch:62 Training Loss: 0.616628 Accuracy on the test set: 0.6693\n",
      "Epoch:63 Training Loss: 0.625237 Accuracy on the test set: 0.664\n",
      "Epoch:64 Training Loss: 0.612647 Accuracy on the test set: 0.6805\n",
      "Epoch:65 Training Loss: 0.610759 Accuracy on the test set: 0.6773\n",
      "Epoch:66 Training Loss: 0.624409 Accuracy on the test set: 0.6581\n",
      "Epoch:67 Training Loss: 0.636451 Accuracy on the test set: 0.6495\n",
      "Epoch:68 Training Loss: 0.634822 Accuracy on the test set: 0.6698\n",
      "Epoch:69 Training Loss: 0.622726 Accuracy on the test set: 0.6691\n",
      "Epoch:70 Training Loss: 0.623356 Accuracy on the test set: 0.6728\n",
      "Epoch:71 Training Loss: 0.620295 Accuracy on the test set: 0.6755\n",
      "Epoch:72 Training Loss: 0.616760 Accuracy on the test set: 0.6767\n",
      "Epoch:73 Training Loss: 0.633379 Accuracy on the test set: 0.6585\n",
      "Epoch:74 Training Loss: 0.621160 Accuracy on the test set: 0.6765\n",
      "Epoch:75 Training Loss: 0.615148 Accuracy on the test set: 0.6828\n",
      "Epoch:76 Training Loss: 0.611992 Accuracy on the test set: 0.6828\n",
      "Epoch:77 Training Loss: 0.612817 Accuracy on the test set: 0.6825\n",
      "Epoch:78 Training Loss: 0.612665 Accuracy on the test set: 0.681\n",
      "Epoch:79 Training Loss: 0.616666 Accuracy on the test set: 0.6666\n",
      "Epoch:80 Training Loss: 0.612058 Accuracy on the test set: 0.6821\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 128\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 2\n",
    "epoch = 80\n",
    "lr = 0.003\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_bin\n",
    "y = yrnn_bin\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YskK1CJ4PPyI",
   "metadata": {
    "id": "YskK1CJ4PPyI"
   },
   "source": [
    "#### Ternary case--m18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "NbOf8UvGfQ-Z",
   "metadata": {
    "id": "NbOf8UvGfQ-Z"
   },
   "outputs": [],
   "source": [
    "# Xrnn_all = np.load(\"./own/xrnnall.npy\")\n",
    "# yrnn_all = np.load(\"./own/yrnnall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "skCShH0pWbur",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "skCShH0pWbur",
    "outputId": "cc91ce78-1afd-48c7-c63d-96c7b8c83642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 1.076744 Accuracy on the test set: 0.4252\n",
      "Epoch:2 Training Loss: 1.057025 Accuracy on the test set: 0.4332\n",
      "Epoch:3 Training Loss: 1.055142 Accuracy on the test set: 0.4369\n",
      "Epoch:4 Training Loss: 1.053933 Accuracy on the test set: 0.4403\n",
      "Epoch:5 Training Loss: 1.052693 Accuracy on the test set: 0.4428\n",
      "Epoch:6 Training Loss: 1.051418 Accuracy on the test set: 0.4455\n",
      "Epoch:7 Training Loss: 1.050142 Accuracy on the test set: 0.4513\n",
      "Epoch:8 Training Loss: 1.048922 Accuracy on the test set: 0.4536\n",
      "Epoch:9 Training Loss: 1.047815 Accuracy on the test set: 0.454\n",
      "Epoch:10 Training Loss: 1.046859 Accuracy on the test set: 0.4572\n",
      "Epoch:11 Training Loss: 1.046049 Accuracy on the test set: 0.4582\n",
      "Epoch:12 Training Loss: 1.045348 Accuracy on the test set: 0.4596\n",
      "Epoch:13 Training Loss: 1.044710 Accuracy on the test set: 0.4597\n",
      "Epoch:14 Training Loss: 1.044087 Accuracy on the test set: 0.46\n",
      "Epoch:15 Training Loss: 1.043439 Accuracy on the test set: 0.4626\n",
      "Epoch:16 Training Loss: 1.042720 Accuracy on the test set: 0.4649\n",
      "Epoch:17 Training Loss: 1.041837 Accuracy on the test set: 0.4659\n",
      "Epoch:18 Training Loss: 1.040500 Accuracy on the test set: 0.4706\n",
      "Epoch:19 Training Loss: 1.036925 Accuracy on the test set: 0.4886\n",
      "Epoch:20 Training Loss: 1.030542 Accuracy on the test set: 0.4927\n",
      "Epoch:21 Training Loss: 1.024596 Accuracy on the test set: 0.5025\n",
      "Epoch:22 Training Loss: 1.018502 Accuracy on the test set: 0.5125\n",
      "Epoch:23 Training Loss: 1.011544 Accuracy on the test set: 0.5185\n",
      "Epoch:24 Training Loss: 1.004216 Accuracy on the test set: 0.5277\n",
      "Epoch:25 Training Loss: 0.998564 Accuracy on the test set: 0.5324\n",
      "Epoch:26 Training Loss: 0.994209 Accuracy on the test set: 0.5375\n",
      "Epoch:27 Training Loss: 0.990553 Accuracy on the test set: 0.5408\n",
      "Epoch:28 Training Loss: 0.986757 Accuracy on the test set: 0.544\n",
      "Epoch:29 Training Loss: 0.983011 Accuracy on the test set: 0.5479\n",
      "Epoch:30 Training Loss: 0.979477 Accuracy on the test set: 0.5492\n",
      "Epoch:31 Training Loss: 0.976093 Accuracy on the test set: 0.5494\n",
      "Epoch:32 Training Loss: 0.972534 Accuracy on the test set: 0.5513\n",
      "Epoch:33 Training Loss: 0.968908 Accuracy on the test set: 0.5533\n",
      "Epoch:34 Training Loss: 0.965400 Accuracy on the test set: 0.5561\n",
      "Epoch:35 Training Loss: 0.961604 Accuracy on the test set: 0.558\n",
      "Epoch:36 Training Loss: 0.957872 Accuracy on the test set: 0.5622\n",
      "Epoch:37 Training Loss: 0.954015 Accuracy on the test set: 0.5637\n",
      "Epoch:38 Training Loss: 0.950181 Accuracy on the test set: 0.5647\n",
      "Epoch:39 Training Loss: 0.946142 Accuracy on the test set: 0.5669\n",
      "Epoch:40 Training Loss: 0.942526 Accuracy on the test set: 0.5705\n",
      "Epoch:41 Training Loss: 0.938600 Accuracy on the test set: 0.5733\n",
      "Epoch:42 Training Loss: 0.934562 Accuracy on the test set: 0.5775\n",
      "Epoch:43 Training Loss: 0.930191 Accuracy on the test set: 0.5778\n",
      "Epoch:44 Training Loss: 0.926219 Accuracy on the test set: 0.5842\n",
      "Epoch:45 Training Loss: 0.921376 Accuracy on the test set: 0.5848\n",
      "Epoch:46 Training Loss: 0.917858 Accuracy on the test set: 0.5873\n",
      "Epoch:47 Training Loss: 0.912892 Accuracy on the test set: 0.5964\n",
      "Epoch:48 Training Loss: 0.906003 Accuracy on the test set: 0.5989\n",
      "Epoch:49 Training Loss: 0.901410 Accuracy on the test set: 0.6065\n",
      "Epoch:50 Training Loss: 0.899903 Accuracy on the test set: 0.6022\n",
      "Epoch:51 Training Loss: 0.979901 Accuracy on the test set: 0.575\n",
      "Epoch:52 Training Loss: 0.942489 Accuracy on the test set: 0.593\n",
      "Epoch:53 Training Loss: 0.907847 Accuracy on the test set: 0.6039\n",
      "Epoch:54 Training Loss: 0.894671 Accuracy on the test set: 0.6144\n",
      "Epoch:55 Training Loss: 0.904487 Accuracy on the test set: 0.5851\n",
      "Epoch:56 Training Loss: 0.901361 Accuracy on the test set: 0.6199\n",
      "Epoch:57 Training Loss: 0.921517 Accuracy on the test set: 0.5947\n",
      "Epoch:58 Training Loss: 0.927097 Accuracy on the test set: 0.6106\n",
      "Epoch:59 Training Loss: 0.905892 Accuracy on the test set: 0.6133\n",
      "Epoch:60 Training Loss: 0.897636 Accuracy on the test set: 0.6254\n",
      "Epoch:61 Training Loss: 0.926051 Accuracy on the test set: 0.622\n",
      "Epoch:62 Training Loss: 0.985271 Accuracy on the test set: 0.5633\n",
      "Epoch:63 Training Loss: 0.947695 Accuracy on the test set: 0.6017\n",
      "Epoch:64 Training Loss: 0.991198 Accuracy on the test set: 0.5105\n",
      "Epoch:65 Training Loss: 0.974550 Accuracy on the test set: 0.4812\n",
      "Epoch:66 Training Loss: 1.016545 Accuracy on the test set: 0.4963\n",
      "Epoch:67 Training Loss: 1.012493 Accuracy on the test set: 0.494\n",
      "Epoch:68 Training Loss: 1.007510 Accuracy on the test set: 0.5039\n",
      "Epoch:69 Training Loss: 0.996000 Accuracy on the test set: 0.5261\n",
      "Epoch:70 Training Loss: 0.989693 Accuracy on the test set: 0.5343\n",
      "Epoch:71 Training Loss: 0.983492 Accuracy on the test set: 0.549\n",
      "Epoch:72 Training Loss: 0.978053 Accuracy on the test set: 0.5503\n",
      "Epoch:73 Training Loss: 0.972288 Accuracy on the test set: 0.5572\n",
      "Epoch:74 Training Loss: 0.984984 Accuracy on the test set: 0.4469\n",
      "Epoch:75 Training Loss: 1.032008 Accuracy on the test set: 0.452\n",
      "Epoch:76 Training Loss: 1.026192 Accuracy on the test set: 0.4568\n",
      "Epoch:77 Training Loss: 1.023500 Accuracy on the test set: 0.4576\n",
      "Epoch:78 Training Loss: 1.021655 Accuracy on the test set: 0.4909\n",
      "Epoch:79 Training Loss: 1.010225 Accuracy on the test set: 0.4984\n",
      "Epoch:80 Training Loss: 1.002314 Accuracy on the test set: 0.5235\n",
      "Epoch:81 Training Loss: 0.988575 Accuracy on the test set: 0.5376\n",
      "Epoch:82 Training Loss: 0.993186 Accuracy on the test set: 0.5138\n",
      "Epoch:83 Training Loss: 0.993256 Accuracy on the test set: 0.5166\n",
      "Epoch:84 Training Loss: 0.989967 Accuracy on the test set: 0.5229\n",
      "Epoch:85 Training Loss: 0.985495 Accuracy on the test set: 0.5462\n",
      "Epoch:86 Training Loss: 1.003089 Accuracy on the test set: 0.5098\n",
      "Epoch:87 Training Loss: 1.023032 Accuracy on the test set: 0.5199\n",
      "Epoch:88 Training Loss: 1.012218 Accuracy on the test set: 0.5384\n",
      "Epoch:89 Training Loss: 1.004276 Accuracy on the test set: 0.5459\n",
      "Epoch:90 Training Loss: 0.998667 Accuracy on the test set: 0.5503\n",
      "Epoch:91 Training Loss: 0.994638 Accuracy on the test set: 0.5542\n",
      "Epoch:92 Training Loss: 0.990362 Accuracy on the test set: 0.562\n",
      "Epoch:93 Training Loss: 0.983124 Accuracy on the test set: 0.5729\n",
      "Epoch:94 Training Loss: 0.979849 Accuracy on the test set: 0.5654\n",
      "Epoch:95 Training Loss: 0.986037 Accuracy on the test set: 0.5615\n",
      "Epoch:96 Training Loss: 0.985870 Accuracy on the test set: 0.5652\n",
      "Epoch:97 Training Loss: 0.984444 Accuracy on the test set: 0.551\n",
      "Epoch:98 Training Loss: 0.995456 Accuracy on the test set: 0.5465\n",
      "Epoch:99 Training Loss: 0.984384 Accuracy on the test set: 0.5453\n",
      "Epoch:100 Training Loss: 0.980388 Accuracy on the test set: 0.5605\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 64\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 3\n",
    "epoch = 60\n",
    "lr = 0.002\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_all\n",
    "y = yrnn_all\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FrmSPfcmPLH_",
   "metadata": {
    "id": "FrmSPfcmPLH_"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fexjQqs5PSgf",
   "metadata": {
    "id": "fexjQqs5PSgf"
   },
   "source": [
    "#### Binary case--m19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YO4DZwQ0Wftu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YO4DZwQ0Wftu",
    "outputId": "9bb56d86-3875-45a9-e97c-487d31bd6d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 0.688657 Accuracy on the test set: 0.547\n",
      "Epoch:2 Training Loss: 0.678230 Accuracy on the test set: 0.5682\n",
      "Epoch:3 Training Loss: 0.666195 Accuracy on the test set: 0.5707\n",
      "Epoch:4 Training Loss: 0.670846 Accuracy on the test set: 0.5152\n",
      "Epoch:5 Training Loss: 0.708411 Accuracy on the test set: 0.5215\n",
      "Epoch:6 Training Loss: 0.650411 Accuracy on the test set: 0.7315\n",
      "Epoch:7 Training Loss: 0.476969 Accuracy on the test set: 0.8215\n",
      "Epoch:8 Training Loss: 0.383537 Accuracy on the test set: 0.844\n",
      "Epoch:9 Training Loss: 0.339800 Accuracy on the test set: 0.8425\n",
      "Epoch:10 Training Loss: 0.310801 Accuracy on the test set: 0.8382\n",
      "Epoch:11 Training Loss: 0.289215 Accuracy on the test set: 0.8442\n",
      "Epoch:12 Training Loss: 0.271746 Accuracy on the test set: 0.8422\n",
      "Epoch:13 Training Loss: 0.252675 Accuracy on the test set: 0.843\n",
      "Epoch:14 Training Loss: 0.234210 Accuracy on the test set: 0.84\n",
      "Epoch:15 Training Loss: 0.217016 Accuracy on the test set: 0.8355\n",
      "Epoch:16 Training Loss: 0.201079 Accuracy on the test set: 0.8385\n",
      "Epoch:17 Training Loss: 0.189909 Accuracy on the test set: 0.834\n",
      "Epoch:18 Training Loss: 0.171364 Accuracy on the test set: 0.8447\n",
      "Epoch:19 Training Loss: 0.170371 Accuracy on the test set: 0.8297\n",
      "Epoch:20 Training Loss: 0.180167 Accuracy on the test set: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 32\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 2\n",
    "epoch = 20\n",
    "lr = 0.1\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()  \n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_bin\n",
    "y = yrnn_bin\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NFTWz42dPVME",
   "metadata": {
    "id": "NFTWz42dPVME"
   },
   "source": [
    "#### Ternary case--m20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DvIV9hntjuJF",
   "metadata": {
    "id": "DvIV9hntjuJF"
   },
   "outputs": [],
   "source": [
    "# Xrnn_all = np.load(\"./own/xrnnall.npy\")\n",
    "# yrnn_all = np.load(\"./own/yrnnall.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWCe1Q2lWjsB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWCe1Q2lWjsB",
    "outputId": "c8c57518-6059-4a2b-8e17-8be2997bda23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss: 1.051962 Accuracy on the test set: 0.442\n",
      "Epoch:2 Training Loss: 1.041174 Accuracy on the test set: 0.4776\n",
      "Epoch:3 Training Loss: 1.031727 Accuracy on the test set: 0.4132\n",
      "Epoch:4 Training Loss: 0.987037 Accuracy on the test set: 0.5622\n",
      "Epoch:5 Training Loss: 0.987023 Accuracy on the test set: 0.5234\n",
      "Epoch:6 Training Loss: 0.986022 Accuracy on the test set: 0.4008\n",
      "Epoch:7 Training Loss: 1.007021 Accuracy on the test set: 0.4\n",
      "Epoch:8 Training Loss: 1.034489 Accuracy on the test set: 0.4056\n",
      "Epoch:9 Training Loss: 0.945747 Accuracy on the test set: 0.6342\n",
      "Epoch:10 Training Loss: 0.807497 Accuracy on the test set: 0.6626\n",
      "Epoch:11 Training Loss: 0.750493 Accuracy on the test set: 0.6842\n",
      "Epoch:12 Training Loss: 0.705669 Accuracy on the test set: 0.6902\n",
      "Epoch:13 Training Loss: 0.669770 Accuracy on the test set: 0.6838\n",
      "Epoch:14 Training Loss: 0.639800 Accuracy on the test set: 0.6766\n",
      "Epoch:15 Training Loss: 0.611865 Accuracy on the test set: 0.6698\n",
      "Epoch:16 Training Loss: 0.588403 Accuracy on the test set: 0.6756\n",
      "Epoch:17 Training Loss: 0.562658 Accuracy on the test set: 0.6758\n",
      "Epoch:18 Training Loss: 0.538898 Accuracy on the test set: 0.6722\n",
      "Epoch:19 Training Loss: 0.516164 Accuracy on the test set: 0.6772\n",
      "Epoch:20 Training Loss: 0.496092 Accuracy on the test set: 0.6734\n",
      "Epoch:21 Training Loss: 0.476217 Accuracy on the test set: 0.6698\n",
      "Epoch:22 Training Loss: 0.453512 Accuracy on the test set: 0.6678\n",
      "Epoch:23 Training Loss: 0.436572 Accuracy on the test set: 0.6666\n",
      "Epoch:24 Training Loss: 0.433279 Accuracy on the test set: 0.6596\n",
      "Epoch:25 Training Loss: 0.414708 Accuracy on the test set: 0.662\n",
      "Epoch:26 Training Loss: 0.409130 Accuracy on the test set: 0.6594\n",
      "Epoch:27 Training Loss: 0.394447 Accuracy on the test set: 0.6662\n",
      "Epoch:28 Training Loss: 0.386139 Accuracy on the test set: 0.6608\n",
      "Epoch:29 Training Loss: 0.379524 Accuracy on the test set: 0.6558\n",
      "Epoch:30 Training Loss: 0.376411 Accuracy on the test set: 0.6554\n",
      "Epoch:31 Training Loss: 0.380072 Accuracy on the test set: 0.6238\n",
      "Epoch:32 Training Loss: 0.546951 Accuracy on the test set: 0.6654\n",
      "Epoch:33 Training Loss: 0.523834 Accuracy on the test set: 0.6534\n",
      "Epoch:34 Training Loss: 0.494883 Accuracy on the test set: 0.6616\n",
      "Epoch:35 Training Loss: 0.441076 Accuracy on the test set: 0.6566\n",
      "Epoch:36 Training Loss: 0.429364 Accuracy on the test set: 0.6518\n",
      "Epoch:37 Training Loss: 0.391016 Accuracy on the test set: 0.6506\n",
      "Epoch:38 Training Loss: 0.352206 Accuracy on the test set: 0.6428\n",
      "Epoch:39 Training Loss: 0.338010 Accuracy on the test set: 0.6566\n",
      "Epoch:40 Training Loss: 0.329700 Accuracy on the test set: 0.6444\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 32\n",
    "n_hidden = 50\n",
    "input_size = 300\n",
    "n_categories = 3\n",
    "epoch = 40\n",
    "lr = 0.1\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.label = y\n",
    "        self.data = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output[:,-1,:].squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "def evaluate():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    hidden_e = None\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_generator:\n",
    "            data = Variable(data).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "            y_pred, hidden = rnn(data, hidden_e)\n",
    "            pred = y_pred.max(1,keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        print('Accuracy on the test set: %.6s' % (correct / total))\n",
    "    \n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size, 'num_workers': 0, 'shuffle': True}\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "X = Xrnn_all\n",
    "y = yrnn_all\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Dataset(X_train, y_train)\n",
    "train_generator = data.DataLoader(train_set, batch_size = batch_size)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "rnn = RNN(input_size, n_hidden, n_categories).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "hidden = torch.zeros(1,batch_size, n_hidden).cuda()\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, label in train_generator:\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad() \n",
    "        y_pred, hidden = rnn(data, hidden)\n",
    "        hidden = hidden.detach()\n",
    "#         label = torch.Tensor(label).long()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(train_generator.dataset)\n",
    "    print('Epoch:{} Training Loss: {:.6f}'.format(e + 1, train_loss), end = \" \")\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa3ea1",
   "metadata": {
    "id": "6faa3ea1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CS544_HW2.2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
