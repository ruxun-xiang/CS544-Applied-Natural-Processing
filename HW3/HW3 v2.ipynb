{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50c287dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e8b21",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef0b8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of my vocabulary:43195\n",
      "Total occurrences of the special token <unk>:20011\n"
     ]
    }
   ],
   "source": [
    "# record occurrence of every word\n",
    "data_path = \"./data/train\"\n",
    "f = open(data_path, \"r\")\n",
    "\n",
    "word_dic = {}\n",
    "unk = \"<unk>\"\n",
    "unknum = \"<unkn>\"\n",
    "unkword = \"<unkw>\"\n",
    "tag_list = []\n",
    "for line in f:\n",
    "    if line != \"\\n\":\n",
    "        index, word, tag = line.split(\"\\t\")\n",
    "        word_dic[word] = word_dic.get(word, 0) + 1\n",
    "\n",
    "# create a new train set with word occurred < 2 replaced by <unk> tag\n",
    "data_path = \"./data/train\"\n",
    "f = open(data_path, \"r\")\n",
    "output_path = \"./data/train_rep_unk\"\n",
    "out = open(output_path, \"w\")\n",
    "\n",
    "for line in f:\n",
    "    if line != \"\\n\":\n",
    "        index, word, tag = line.split(\"\\t\")\n",
    "        if word_dic[word] < 2:\n",
    "            # classify the unknown words\n",
    "            if \"0\" < word < \"9\":\n",
    "                out.write(index + \"\\t\" + unknum + \"\\t\" + tag) \n",
    "            else:\n",
    "                out.write(index + \"\\t\" + unkword + \"\\t\" + tag)\n",
    "        out.write(index + \"\\t\" + word + \"\\t\" + tag)\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        out.write(\"\\n\")\n",
    "\n",
    "# use the new train and calculate word occurrence again(including unk)\n",
    "data_path = \"./data/train_rep_unk\"\n",
    "f = open(data_path)\n",
    "\n",
    "word_dic_unk = {}\n",
    "for line in f:\n",
    "    if line != \"\\n\":\n",
    "        index, word, tag = line.split(\"\\t\")\n",
    "        tag = tag.split(\"\\n\")[0]\n",
    "        word_dic_unk[word] = word_dic_unk.get(word, 0) + 1\n",
    "        if tag not in tag_list:\n",
    "            tag_list.append(tag)\n",
    "f.close()\n",
    "\n",
    "print(\"Total size of my vocabulary:{}\".format(len(word_dic_unk)))\n",
    "print(\"Total occurrences of the special token <unk>:{}\".format(word_dic_unk[unknum] + word_dic_unk[unkword]))\n",
    "\n",
    "# reorder the words based on their occurrence\n",
    "word_dic_order = sorted(word_dic_unk.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# create vocab.txt using the ordered vocab\n",
    "index = 1\n",
    "output_file = \"./data/vocab.txt\"\n",
    "f = open(output_file, \"w\")\n",
    "f.write(unk + \"\\t\" + str(index) + \"\\t\" + str(word_dic_unk[unkword] + word_dic_unk[unknum]))\n",
    "f.write(\"\\n\")\n",
    "index += 1\n",
    "for word, occur in word_dic_order:\n",
    "    if word != unknum and word != unkword:\n",
    "        f.write(word + \"\\t\" + str(index) + \"\\t\" + str(occur))\n",
    "        f.write(\"\\n\")\n",
    "        index += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d36df7",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59e378bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of transition parameters:1398\n",
      "Total number of emission parameters:50319\n"
     ]
    }
   ],
   "source": [
    "# create hmm.json\n",
    "train_file = \"./data/train_rep_unk\"\n",
    "f = open(train_file)\n",
    "\n",
    "cnt_s1s2 = {}\n",
    "cnt_sx = {}\n",
    "\n",
    "cnt_s = {}\n",
    "\n",
    "start_tag = \"<s>\"\n",
    "\n",
    "# compute transition and emission\n",
    "def compute_ts(sentence):\n",
    "    sent_len = len(sentence)\n",
    "    for i in range(sent_len):\n",
    "        if sent_len == 1:\n",
    "            first_i, first_x, first_s = sentence[i].split(\"\\n\")[0].split(\"\\t\")\n",
    "            cnt_s1s2[(start_tag, first_s)] = cnt_s1s2.get((start_tag, first_s), 0) + 1\n",
    "            cnt_sx[(first_s, first_x)] = cnt_sx.get((first_s, first_x), 0) + 1\n",
    "            cnt_s[first_s] = cnt_s.get(first_s, 0) + 1\n",
    "            cnt_s[start_tag] = cnt_s.get(start_tag, 0) + 1\n",
    "\n",
    "        if i + 1 < sent_len:\n",
    "            j = i + 1\n",
    "            first_i, first_x, first_s = sentence[i].split(\"\\n\")[0].split(\"\\t\")\n",
    "            sec_i, sec_x, sec_s = sentence[j].split(\"\\n\")[0].split(\"\\t\")\n",
    "\n",
    "            cnt_s1s2[(first_s, sec_s)] = cnt_s1s2.get((first_s, sec_s), 0) + 1\n",
    "            cnt_sx[(first_s, first_x)] = cnt_sx.get((first_s, first_x), 0) + 1\n",
    "            cnt_s[first_s] = cnt_s.get(first_s, 0) + 1\n",
    "            # if it's first word, append a key (<s>, first_word)\n",
    "            if i == 0:\n",
    "                cnt_s1s2[(start_tag, first_s)] = cnt_s1s2.get((start_tag, first_s), 0) + 1\n",
    "                cnt_s[start_tag] = cnt_s.get(start_tag, 0) + 1\n",
    "            if j == sent_len - 1:\n",
    "                cnt_s[sec_s] = cnt_s.get(sec_s, 0) + 1\n",
    "                cnt_sx[(sec_s, sec_x)] = cnt_sx.get((sec_s, sec_x), 0) + 1\n",
    "\n",
    "sentence = []\n",
    "for line in f:\n",
    "    if line != \"\\n\":\n",
    "        sentence.append(line)\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        compute_ts(sentence)\n",
    "        sentence = []\n",
    "f.close()\n",
    "\n",
    "# record string key for storation in json file\n",
    "# use original key for later computation\n",
    "transition_str = {}\n",
    "transition = {}\n",
    "emission_str = {}\n",
    "emission = {}\n",
    "\n",
    "ts = {}\n",
    "alpha = 0.001\n",
    "for key, value in cnt_s1s2.items():\n",
    "    s1 = key[0]\n",
    "    s2 = key[1]\n",
    "    s1s2 = \"(\" + s1 + \",\" + s2 + \")\"\n",
    "    transition_str[s1s2] = (value + alpha) / ( cnt_s[key[0]] + alpha * len(cnt_s1s2))\n",
    "    # smoothing\n",
    "    transition[key] = (value + alpha) / ( cnt_s[key[0]] + alpha * len(cnt_s1s2))\n",
    "\n",
    "for key, value in cnt_sx.items():\n",
    "    s = key[0]\n",
    "    x = key[1]\n",
    "    sx = \"(\" + s + \",\" + x + \")\"\n",
    "    emission_str[sx] = (value + alpha) / ( cnt_s[key[0]] + alpha * len(cnt_sx))\n",
    "    # smoothing\n",
    "    emission[key] = (value + alpha) / ( cnt_s[key[0]] + alpha * len(cnt_sx))\n",
    "\n",
    "print(\"Total number of transition parameters:{}\".format(len(transition)))\n",
    "print(\"Total number of emission parameters:{}\".format(len(emission)))\n",
    "\n",
    "hmm = {}\n",
    "hmm['transition'] = transition_str\n",
    "hmm['emission'] = emission_str\n",
    "hmm_file = \"./data/hmm.json\"\n",
    "fout = open(hmm_file, \"w\")\n",
    "fout.write(json.dumps(hmm))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8eb1c",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3680e0",
   "metadata": {},
   "source": [
    "## Greedy for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec278830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_acc(sentence):\n",
    "    s = []\n",
    "    s_t = []\n",
    "    curr_tag = \"\"\n",
    "    correct = 0\n",
    "    sent_len = len(sentence)\n",
    "    for i in range(sent_len):\n",
    "        maxp = -1\n",
    "        index, word, wtag = sentence[i].split(\"\\t\")\n",
    "        wtag = wtag.split(\"\\n\")[0]\n",
    "        if word not in word_dic_unk.keys():\n",
    "            if \"0\" < word < \"9\":\n",
    "                word = unknum\n",
    "            else:\n",
    "                word = unkword\n",
    "\n",
    "        # if first word, s_prime should be <s>\n",
    "        if i == 0:\n",
    "            s_prime = start_tag\n",
    "        else:\n",
    "            s_prime = s[i - 1]\n",
    "\n",
    "        pred_tag = \"\"\n",
    "        for tag in tag_list:\n",
    "            t_val = 0\n",
    "            e_val = 0\n",
    "            e_key = (tag, word)\n",
    "            t_key = (s_prime, tag)\n",
    "            # for all possible situations, first find suitable t\n",
    "            if t_key in transition.keys():\n",
    "                t_val = transition[t_key]\n",
    "                # then find suitable e\n",
    "                if e_key in emission.keys():\n",
    "                    e_val = emission[e_key]\n",
    "            p = t_val * e_val\n",
    "            # record max p, record the corresponding predicted tag\n",
    "            if p > maxp:\n",
    "                maxp = p\n",
    "                pred_tag = tag\n",
    "        s.append(pred_tag)\n",
    "        # compute correct prediction\n",
    "        if pred_tag == wtag:\n",
    "            correct += 1\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243a3f6",
   "metadata": {},
   "source": [
    "## Evaluate greedy on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd6adea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding accuracy on dev data:94.3%\n"
     ]
    }
   ],
   "source": [
    "dev_file = \"./data/dev\"\n",
    "f = open(dev_file)\n",
    "sentence = []\n",
    "corrects = 0\n",
    "length = 0\n",
    "counter = 0\n",
    "dev_count = 0\n",
    "for line in f:\n",
    "    dev_count += 1\n",
    "    if line != \"\\n\":\n",
    "        sentence.append(line)\n",
    "\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        length += len(sentence)\n",
    "        correct = greedy_acc(sentence)\n",
    "        corrects += correct\n",
    "        sentence = []\n",
    "        counter += 1\n",
    "\n",
    "print(\"Greedy decoding accuracy on dev data:{:.1f}%\".format(corrects * 100 / length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7bc56",
   "metadata": {},
   "source": [
    "## Greedy for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bbe90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def greedy_pos(sentence, fout):\n",
    "    s = []\n",
    "    s_t = []\n",
    "    curr_tag = \"\"\n",
    "    correct = 0\n",
    "    sent_len = len(sentence)\n",
    "    for i in range(sent_len):\n",
    "        maxp = -1\n",
    "        index, word = sentence[i].split(\"\\t\")\n",
    "        word = word.split(\"\\n\")[0]\n",
    "        ori_word = word\n",
    "        if word not in word_dic_unk.keys():\n",
    "            if \"0\" < word < \"9\":\n",
    "                word = unknum\n",
    "            else:\n",
    "                word = unkword\n",
    "\n",
    "        if i == 0:\n",
    "            s_prime = start_tag\n",
    "        else:\n",
    "            s_prime = s[i - 1]\n",
    "\n",
    "        pred_tag = \"\"\n",
    "        for tag in tag_list:\n",
    "            t_val = 0\n",
    "            e_val = 0\n",
    "            e_key = (tag, word)\n",
    "            t_key = (s_prime, tag)\n",
    "\n",
    "            if t_key in transition.keys():\n",
    "                t_val = transition[t_key]\n",
    "                if e_key in emission.keys():\n",
    "                    e_val = emission[e_key]\n",
    "            p = t_val * e_val\n",
    "            if p > maxp:\n",
    "                maxp = p\n",
    "                pred_tag = tag\n",
    "        s.append(pred_tag)\n",
    "        # write prediction in file\n",
    "        fout.write(index + \"\\t\" + ori_word + \"\\t\" + pred_tag)\n",
    "        fout.write(\"\\n\")\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd368a",
   "metadata": {},
   "source": [
    "## Predict POS in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b474c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"./data/test\"\n",
    "f = open(test_file)\n",
    "output_file = \"./data/greedy.out\"\n",
    "fout = open(output_file, \"w\")\n",
    "sentence = []\n",
    "corrects = 0\n",
    "length = 0\n",
    "counter = 0\n",
    "test_count = 0\n",
    "for line in f:\n",
    "    test_count += 1\n",
    "    if line != \"\\n\":\n",
    "        sentence.append(line)\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        flag = greedy_pos(sentence, fout)\n",
    "        fout.write(\"\\n\")\n",
    "        sentence = []\n",
    "\n",
    "f.close()\n",
    "fout.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3850fa",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb7e56",
   "metadata": {},
   "source": [
    "## Viterbi for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "883e77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_acc(sentence):\n",
    "    pi = {}\n",
    "    path = {}\n",
    "    best = \"\"\n",
    "    sent_len = len(sentence)\n",
    "    wtags = []\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(sent_len):\n",
    "        pi_val = {}\n",
    "        pi_path = {}\n",
    "        index, word, wtag = sentence[i].split(\"\\t\")\n",
    "        wtag = wtag.split(\"\\n\")[0]\n",
    "        wtags.append(wtag)\n",
    "\n",
    "        if word not in word_dic_unk.keys():\n",
    "            if \"0\" < word < \"9\":\n",
    "                word = unknum\n",
    "            else:\n",
    "                word = unkword\n",
    "\n",
    "        if i == 0:\n",
    "            s_prime = start_tag\n",
    "            for tag in tag_list:\n",
    "                t_val = 0\n",
    "                e_val = 0\n",
    "                e_key = (tag, word)\n",
    "                t_key = (s_prime, tag)\n",
    "                # first find suitable t\n",
    "                if t_key in transition.keys():\n",
    "                    t_val = transition[t_key]\n",
    "                    # then find suitable e\n",
    "                    if e_key in emission.keys():\n",
    "                        e_val = emission[e_key]\n",
    "                # record pi value and path that led to this value\n",
    "                pi_val[tag] = t_val * e_val\n",
    "                pi_path[tag] = [tag]\n",
    "            # record the first layer pi value and path\n",
    "            pi[i] = pi_val\n",
    "            path[i] = pi_path\n",
    "\n",
    "        else:\n",
    "            for curr_tag in tag_list:\n",
    "                pi_val[curr_tag] = -1\n",
    "\n",
    "                for prev_tag in tag_list:\n",
    "                    prev_pi = pi[i - 1][prev_tag]\n",
    "\n",
    "                    curr_pi = 0\n",
    "\n",
    "                    if prev_pi != 0:\n",
    "                        t_val = 0\n",
    "                        e_val = 0\n",
    "                        e_key = (curr_tag, word)\n",
    "                        t_key = (prev_tag, curr_tag)\n",
    "\n",
    "                        if t_key in transition.keys():\n",
    "                            t_val = transition[t_key]\n",
    "                            if e_key in emission.keys():\n",
    "                                e_val = emission[e_key]\n",
    "                        # record current pi for later comparison\n",
    "                        curr_pi = prev_pi * t_val * e_val\n",
    "                    # if for the tag in this layer, current pi value greater than max in record, \n",
    "                    # replace and record the current pi value and the path that led to this value\n",
    "                    if curr_pi > pi_val[curr_tag]:\n",
    "                        pi_val[curr_tag] = curr_pi\n",
    "                        pi_path[curr_tag] = path[i - 1][prev_tag][:]\n",
    "                        pi_path[curr_tag].append(curr_tag)\n",
    "            # record the pi values and paths for this layer\n",
    "            pi[i] = pi_val\n",
    "            path[i] = pi_path\n",
    "    # find the best pi value in the last layer, and the corresponding path\n",
    "    best_pi = max(pi[sent_len - 1], key=pi[sent_len - 1].get)\n",
    "    best_path = path[sent_len - 1][best_pi]\n",
    "    # compute correct prediction\n",
    "    for i in range(len(best_path)):\n",
    "        if best_path[i] == wtags[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a2c02",
   "metadata": {},
   "source": [
    "## Evaluate Viterbi on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "955bd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 137294/137294 [00:55<00:00, 2468.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding accuracy on dev data:95.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(dev_file)\n",
    "sentence = []\n",
    "corrects = 0\n",
    "length = 0\n",
    "counter = 0\n",
    "for line in tqdm(f, total=dev_count):\n",
    "    if line != \"\\n\":\n",
    "        sentence.append(line)\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        length += len(sentence)\n",
    "        correct = viterbi_acc(sentence)\n",
    "        corrects += correct\n",
    "        sentence = []\n",
    "\n",
    "print(\"Viterbi decoding accuracy on dev data:{:.1f}%\".format(corrects * 100 / length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab86cbe",
   "metadata": {},
   "source": [
    "## Viterbi for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9a85bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_pos(sentence, fout):\n",
    "    pi = {}\n",
    "    path = {}\n",
    "    best = \"\"\n",
    "    sent_len = len(sentence)\n",
    "    words = []\n",
    "    correct = 0\n",
    "\n",
    "\n",
    "    for i in range(sent_len):\n",
    "        pi_val = {}\n",
    "        pi_path = {}\n",
    "        index, word = sentence[i].split(\"\\t\")\n",
    "        word = word.split(\"\\n\")[0]\n",
    "        words.append(word)\n",
    "\n",
    "        if word not in word_dic_unk.keys():\n",
    "            if \"0\" < word < \"9\":\n",
    "                word = unknum\n",
    "            else:\n",
    "                word = unkword\n",
    "\n",
    "        if i == 0:\n",
    "            s_prime = start_tag\n",
    "            for tag in tag_list:\n",
    "                t_val = 0\n",
    "                e_val = 0\n",
    "                e_key = (tag, word)\n",
    "                t_key = (s_prime, tag)\n",
    "                if t_key in transition.keys():\n",
    "                    t_val = transition[t_key]\n",
    "                    if e_key in emission.keys():\n",
    "                        e_val = emission[e_key]\n",
    "                pi_val[tag] = t_val * e_val\n",
    "                pi_path[tag] = [tag]\n",
    "            pi[i] = pi_val\n",
    "            path[i] = pi_path\n",
    "\n",
    "        else:\n",
    "            for curr_tag in tag_list:\n",
    "                pi_val[curr_tag] = -1\n",
    "\n",
    "                for prev_tag in tag_list:\n",
    "                    prev_pi = pi[i - 1][prev_tag]\n",
    "\n",
    "                    curr_pi = 0\n",
    "\n",
    "                    if prev_pi != 0:\n",
    "                        t_val = 0\n",
    "                        e_val = 0\n",
    "                        e_key = (curr_tag, word)\n",
    "                        t_key = (prev_tag, curr_tag)\n",
    "\n",
    "                        if t_key in transition.keys():\n",
    "                            t_val = transition[t_key]\n",
    "                            if e_key in emission.keys():\n",
    "                                e_val = emission[e_key]\n",
    "\n",
    "                        curr_pi = prev_pi * t_val * e_val\n",
    "                    if curr_pi > pi_val[curr_tag]:\n",
    "                        pi_val[curr_tag] = curr_pi\n",
    "                        pi_path[curr_tag] = path[i - 1][prev_tag][:]\n",
    "                        pi_path[curr_tag].append(curr_tag)\n",
    "\n",
    "            pi[i] = pi_val\n",
    "            path[i] = pi_path\n",
    "    best_pi = max(pi[sent_len - 1], key=pi[sent_len - 1].get)\n",
    "\n",
    "    best_path = path[sent_len - 1][best_pi]\n",
    "    # write prediction into file\n",
    "    for i in range(len(best_path)):\n",
    "        fout.write(str(i + 1) + \"\\t\" + words[i] + \"\\t\" + best_path[i])\n",
    "        fout.write(\"\\n\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407704b",
   "metadata": {},
   "source": [
    "## Predict POS in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a07e1ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 135115/135115 [00:56<00:00, 2397.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test_file = \"./data/test\"\n",
    "f = open(test_file)\n",
    "output_file = \"./data/viterbi.out\"\n",
    "fout = open(output_file, \"w\")\n",
    "sentence = []\n",
    "corrects = 0\n",
    "length = 0\n",
    "counter = 0\n",
    "for line in tqdm(f, total=test_count):\n",
    "    if line != \"\\n\":\n",
    "        sentence.append(line)\n",
    "    elif line == \"\\n\" or line == \"\":\n",
    "        flag = viterbi_pos(sentence, fout)\n",
    "        fout.write(\"\\n\")\n",
    "        sentence = []\n",
    "\n",
    "f.close()\n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
